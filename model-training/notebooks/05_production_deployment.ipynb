{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c2871e9",
   "metadata": {},
   "source": [
    "# Production Deployment & Monitoring - School Suspension Prediction\n",
    "\n",
    "**Purpose:** Prepare the model for production deployment with FastAPI and monitoring\n",
    "\n",
    "**Critical:** This notebook creates production artifacts and monitoring tools\n",
    "\n",
    "**Structure:**\n",
    "- Section 0: Setup & load production model\n",
    "- Section 1: Create prediction pipeline\n",
    "- Section 2: Input validation & preprocessing\n",
    "- Section 3: Batch prediction capabilities\n",
    "- Section 4: Model versioning & metadata\n",
    "- Section 5: Performance monitoring tools\n",
    "- Section 6: Generate FastAPI endpoints\n",
    "- Section 7: Docker & deployment artifacts\n",
    "\n",
    "**References:**\n",
    "- 02_eda_and_core_model_all_features.ipynb (training)\n",
    "- 03_final_test_evaluation.ipynb (validation)\n",
    "- ../web/app.py (FastAPI integration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3246de",
   "metadata": {},
   "source": [
    "## Section 0: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce097c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0.1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "import pickle\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Validation\n",
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../data')\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "MODELS_DIR = Path('../models')\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "PRODUCTION_DIR = MODELS_DIR / 'production'\n",
    "PRODUCTION_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# LOCATION_MAPPING\n",
    "LOCATION_MAPPING = {\n",
    "    0: 'Manila', 1: 'Quezon City', 2: 'Caloocan', 3: 'Las PiÃ±as',\n",
    "    4: 'Makati', 5: 'Malabon', 6: 'Mandaluyong', 7: 'Marikina',\n",
    "    8: 'Muntinlupa', 9: 'Navotas', 10: 'ParaÃ±aque', 11: 'Pasay',\n",
    "    12: 'Pasig', 13: 'Pateros', 14: 'San Juan', 15: 'Taguig',\n",
    "    16: 'Valenzuela'\n",
    "}\n",
    "\n",
    "REVERSE_LOCATION_MAPPING = {v: k for k, v in LOCATION_MAPPING.items()}\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "print(f\"Production directory: {PRODUCTION_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0806c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0.2: Load Best Model and Metadata\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING PRODUCTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load metadata\n",
    "with open(str(PROCESSED_DIR / 'core_model_metadata.json'), 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"\\nModel: {metadata['best_model']}\")\n",
    "print(f\"  F2 Score: {metadata['best_f2']:.4f}\")\n",
    "print(f\"  Recall: {metadata['best_recall']:.4f}\")\n",
    "print(f\"  Precision: {metadata['best_precision']:.4f}\")\n",
    "print(f\"  Features: {metadata['final_feature_count']}\")\n",
    "\n",
    "# Load model\n",
    "model_path = PROCESSED_DIR / 'best_core_model.pkl'\n",
    "best_model = joblib.load(str(model_path))\n",
    "print(f\"\\nâœ… Model loaded: {type(best_model).__name__}\")\n",
    "\n",
    "# Store expected features\n",
    "expected_features = metadata['selected_features']\n",
    "print(f\"\\nExpected features: {len(expected_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f9e169",
   "metadata": {},
   "source": [
    "## Section 1: Create Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6186d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.1: Define Pydantic Models for Input Validation\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import Optional\n",
    "from datetime import date\n",
    "\n",
    "class PredictionInput(BaseModel):\n",
    "    \"\"\"Input schema for single prediction request\"\"\"\n",
    "    \n",
    "    # Date and location\n",
    "    date: str = Field(..., description=\"Date in YYYY-MM-DD format\")\n",
    "    lgu_name: str = Field(..., description=\"LGU name (e.g., 'Manila', 'Quezon City')\")\n",
    "    \n",
    "    # Calendar features (can be auto-computed from date)\n",
    "    is_holiday: Optional[int] = Field(None, description=\"1 if holiday, 0 otherwise\")\n",
    "    is_school_day: Optional[int] = Field(None, description=\"1 if school day, 0 otherwise\")\n",
    "    \n",
    "    # Weather features (historical - day before)\n",
    "    hist_precipitation_sum_t1: float = Field(..., description=\"Precipitation sum (mm) - previous day\")\n",
    "    hist_wind_speed_max_t1: float = Field(..., description=\"Max wind speed (km/h) - previous day\")\n",
    "    hist_wind_gusts_max_t1: Optional[float] = Field(None, description=\"Max wind gusts (km/h) - previous day\")\n",
    "    hist_pressure_msl_min_t1: Optional[float] = Field(None, description=\"Min MSL pressure (hPa) - previous day\")\n",
    "    hist_temperature_max_t1: Optional[float] = Field(None, description=\"Max temperature (Â°C) - previous day\")\n",
    "    hist_relative_humidity_mean_t1: Optional[float] = Field(None, description=\"Mean relative humidity (%) - previous day\")\n",
    "    hist_cloud_cover_max_t1: Optional[float] = Field(None, description=\"Max cloud cover (%) - previous day\")\n",
    "    hist_dew_point_mean_t1: Optional[float] = Field(None, description=\"Mean dew point (Â°C) - previous day\")\n",
    "    hist_apparent_temperature_max_t1: Optional[float] = Field(None, description=\"Max apparent temperature (Â°C) - previous day\")\n",
    "    hist_weather_code_t1: Optional[int] = Field(None, description=\"Weather code - previous day\")\n",
    "    \n",
    "    # Weather features (historical aggregations)\n",
    "    hist_precip_sum_7d: Optional[float] = Field(None, description=\"7-day precipitation sum (mm)\")\n",
    "    hist_precip_sum_3d: Optional[float] = Field(None, description=\"3-day precipitation sum (mm)\")\n",
    "    hist_wind_max_7d: Optional[float] = Field(None, description=\"7-day max wind speed (km/h)\")\n",
    "    \n",
    "    # Weather features (forecast - for target day)\n",
    "    fcst_precipitation_sum: float = Field(..., description=\"Forecast precipitation sum (mm)\")\n",
    "    fcst_precipitation_hours: Optional[float] = Field(None, description=\"Forecast precipitation hours\")\n",
    "    fcst_wind_speed_max: float = Field(..., description=\"Forecast max wind speed (km/h)\")\n",
    "    fcst_wind_gusts_max: Optional[float] = Field(None, description=\"Forecast max wind gusts (km/h)\")\n",
    "    fcst_pressure_msl_min: Optional[float] = Field(None, description=\"Forecast min MSL pressure (hPa)\")\n",
    "    fcst_temperature_max: Optional[float] = Field(None, description=\"Forecast max temperature (Â°C)\")\n",
    "    fcst_relative_humidity_mean: Optional[float] = Field(None, description=\"Forecast mean relative humidity (%)\")\n",
    "    fcst_cloud_cover_max: Optional[float] = Field(None, description=\"Forecast max cloud cover (%)\")\n",
    "    fcst_dew_point_mean: Optional[float] = Field(None, description=\"Forecast mean dew point (Â°C)\")\n",
    "    fcst_cape_max: Optional[float] = Field(None, description=\"Forecast max CAPE (J/kg)\")\n",
    "    \n",
    "    # Flood risk\n",
    "    mean_flood_risk_score: Optional[float] = Field(None, description=\"Mean flood risk score for LGU\")\n",
    "    \n",
    "    @validator('date')\n",
    "    def validate_date(cls, v):\n",
    "        try:\n",
    "            pd.to_datetime(v)\n",
    "            return v\n",
    "        except:\n",
    "            raise ValueError(f\"Invalid date format: {v}. Expected YYYY-MM-DD\")\n",
    "    \n",
    "    @validator('lgu_name')\n",
    "    def validate_lgu(cls, v):\n",
    "        if v not in REVERSE_LOCATION_MAPPING:\n",
    "            raise ValueError(f\"Invalid LGU: {v}. Must be one of {list(REVERSE_LOCATION_MAPPING.keys())}\")\n",
    "        return v\n",
    "\n",
    "class PredictionOutput(BaseModel):\n",
    "    \"\"\"Output schema for prediction response\"\"\"\n",
    "    date: str\n",
    "    lgu_name: str\n",
    "    suspension_predicted: int = Field(..., description=\"0 or 1\")\n",
    "    suspension_probability: float = Field(..., description=\"Probability of suspension [0.0-1.0]\")\n",
    "    confidence_level: str = Field(..., description=\"Low, Medium, or High\")\n",
    "    model_version: str\n",
    "    prediction_timestamp: str\n",
    "\n",
    "print(\"âœ… Pydantic models defined\")\n",
    "print(f\"   Input fields: {len(PredictionInput.__fields__)}\")\n",
    "print(f\"   Output fields: {len(PredictionOutput.__fields__)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea210a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.2: Create Preprocessing Pipeline\n",
    "class PredictionPipeline:\n",
    "    \"\"\"\n",
    "    Production-ready prediction pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, metadata, location_mapping):\n",
    "        self.model = model\n",
    "        self.metadata = metadata\n",
    "        self.location_mapping = location_mapping\n",
    "        self.reverse_location_mapping = {v: k for k, v in location_mapping.items()}\n",
    "        self.expected_features = metadata['selected_features']\n",
    "        self.model_version = f\"{metadata['best_model']}_v1.0\"\n",
    "        \n",
    "    def preprocess_input(self, input_data: PredictionInput) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert PredictionInput to model-ready DataFrame\n",
    "        \"\"\"\n",
    "        # Parse date\n",
    "        date_obj = pd.to_datetime(input_data.date)\n",
    "        \n",
    "        # Extract calendar features from date\n",
    "        year = date_obj.year\n",
    "        month = date_obj.month\n",
    "        day = date_obj.day\n",
    "        day_of_week = date_obj.dayofweek\n",
    "        \n",
    "        # Rainy season (June-November)\n",
    "        is_rainy_season = 1 if month in [6, 7, 8, 9, 10, 11] else 0\n",
    "        \n",
    "        # School year start (June = 0, May = 11)\n",
    "        if month >= 6:\n",
    "            month_from_sy_start = month - 6\n",
    "        else:\n",
    "            month_from_sy_start = month + 6\n",
    "        \n",
    "        # Get LGU ID\n",
    "        lgu_id = self.reverse_location_mapping[input_data.lgu_name]\n",
    "        \n",
    "        # Get flood risk (use default if not provided)\n",
    "        mean_flood_risk_score = input_data.mean_flood_risk_score or 2.5\n",
    "        \n",
    "        # Create feature dictionary\n",
    "        features = {\n",
    "            'year': year,\n",
    "            'month': month,\n",
    "            'day': day,\n",
    "            'day_of_week': day_of_week,\n",
    "            'is_rainy_season': is_rainy_season,\n",
    "            'month_from_sy_start': month_from_sy_start,\n",
    "            'is_holiday': input_data.is_holiday or 0,\n",
    "            'is_school_day': input_data.is_school_day or 1,\n",
    "            'lgu_id': lgu_id,\n",
    "            'mean_flood_risk_score': mean_flood_risk_score,\n",
    "            \n",
    "            # Historical weather (t-1)\n",
    "            'hist_precipitation_sum_t1': input_data.hist_precipitation_sum_t1,\n",
    "            'hist_wind_speed_max_t1': input_data.hist_wind_speed_max_t1,\n",
    "            'hist_wind_gusts_max_t1': input_data.hist_wind_gusts_max_t1 or 0,\n",
    "            'hist_pressure_msl_min_t1': input_data.hist_pressure_msl_min_t1 or 1013,\n",
    "            'hist_temperature_max_t1': input_data.hist_temperature_max_t1 or 30,\n",
    "            'hist_relative_humidity_mean_t1': input_data.hist_relative_humidity_mean_t1 or 75,\n",
    "            'hist_cloud_cover_max_t1': input_data.hist_cloud_cover_max_t1 or 50,\n",
    "            'hist_dew_point_mean_t1': input_data.hist_dew_point_mean_t1 or 25,\n",
    "            'hist_apparent_temperature_max_t1': input_data.hist_apparent_temperature_max_t1 or 32,\n",
    "            'hist_weather_code_t1': input_data.hist_weather_code_t1 or 0,\n",
    "            \n",
    "            # Historical aggregations\n",
    "            'hist_precip_sum_7d': input_data.hist_precip_sum_7d or input_data.hist_precipitation_sum_t1 * 3,\n",
    "            'hist_precip_sum_3d': input_data.hist_precip_sum_3d or input_data.hist_precipitation_sum_t1 * 1.5,\n",
    "            'hist_wind_max_7d': input_data.hist_wind_max_7d or input_data.hist_wind_speed_max_t1 * 1.2,\n",
    "            \n",
    "            # Forecast weather\n",
    "            'fcst_precipitation_sum': input_data.fcst_precipitation_sum,\n",
    "            'fcst_precipitation_hours': input_data.fcst_precipitation_hours or 0,\n",
    "            'fcst_wind_speed_max': input_data.fcst_wind_speed_max,\n",
    "            'fcst_wind_gusts_max': input_data.fcst_wind_gusts_max or 0,\n",
    "            'fcst_pressure_msl_min': input_data.fcst_pressure_msl_min or 1013,\n",
    "            'fcst_temperature_max': input_data.fcst_temperature_max or 30,\n",
    "            'fcst_relative_humidity_mean': input_data.fcst_relative_humidity_mean or 75,\n",
    "            'fcst_cloud_cover_max': input_data.fcst_cloud_cover_max or 50,\n",
    "            'fcst_dew_point_mean': input_data.fcst_dew_point_mean or 25,\n",
    "            'fcst_cape_max': input_data.fcst_cape_max or 0,\n",
    "        }\n",
    "        \n",
    "        # Create DataFrame with expected feature order\n",
    "        df = pd.DataFrame([features])\n",
    "        df = df[self.expected_features]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def predict(self, input_data: PredictionInput) -> PredictionOutput:\n",
    "        \"\"\"\n",
    "        Make prediction and return structured output\n",
    "        \"\"\"\n",
    "        # Preprocess\n",
    "        X = self.preprocess_input(input_data)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = self.model.predict(X)[0]\n",
    "        y_proba = self.model.predict_proba(X)[0, 1]\n",
    "        \n",
    "        # Determine confidence level\n",
    "        if y_proba < 0.3 or y_proba > 0.7:\n",
    "            confidence = \"High\"\n",
    "        elif y_proba < 0.4 or y_proba > 0.6:\n",
    "            confidence = \"Medium\"\n",
    "        else:\n",
    "            confidence = \"Low\"\n",
    "        \n",
    "        # Create output\n",
    "        output = PredictionOutput(\n",
    "            date=input_data.date,\n",
    "            lgu_name=input_data.lgu_name,\n",
    "            suspension_predicted=int(y_pred),\n",
    "            suspension_probability=float(y_proba),\n",
    "            confidence_level=confidence,\n",
    "            model_version=self.model_version,\n",
    "            prediction_timestamp=datetime.now().isoformat()\n",
    "        )\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = PredictionPipeline(best_model, metadata, LOCATION_MAPPING)\n",
    "print(\"âœ… Prediction pipeline created\")\n",
    "print(f\"   Model version: {pipeline.model_version}\")\n",
    "print(f\"   Expected features: {len(pipeline.expected_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2df45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.3: Test the Pipeline\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING PREDICTION PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create test input (heavy rain scenario)\n",
    "test_input = PredictionInput(\n",
    "    date=\"2025-07-15\",\n",
    "    lgu_name=\"Manila\",\n",
    "    is_holiday=0,\n",
    "    is_school_day=1,\n",
    "    hist_precipitation_sum_t1=50.0,\n",
    "    hist_wind_speed_max_t1=45.0,\n",
    "    hist_wind_gusts_max_t1=60.0,\n",
    "    fcst_precipitation_sum=80.0,\n",
    "    fcst_wind_speed_max=55.0,\n",
    "    fcst_wind_gusts_max=70.0,\n",
    "    mean_flood_risk_score=4.0\n",
    ")\n",
    "\n",
    "print(\"\\nTest Input (Heavy Rain Scenario):\")\n",
    "print(f\"  Date: {test_input.date}\")\n",
    "print(f\"  LGU: {test_input.lgu_name}\")\n",
    "print(f\"  Historical Precipitation: {test_input.hist_precipitation_sum_t1} mm\")\n",
    "print(f\"  Forecast Precipitation: {test_input.fcst_precipitation_sum} mm\")\n",
    "print(f\"  Flood Risk: {test_input.mean_flood_risk_score}\")\n",
    "\n",
    "# Make prediction\n",
    "result = pipeline.predict(test_input)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREDICTION RESULT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Suspension Predicted: {'YES' if result.suspension_predicted else 'NO'}\")\n",
    "print(f\"  Probability: {result.suspension_probability:.2%}\")\n",
    "print(f\"  Confidence: {result.confidence_level}\")\n",
    "print(f\"  Model Version: {result.model_version}\")\n",
    "print(f\"  Timestamp: {result.prediction_timestamp}\")\n",
    "\n",
    "print(\"\\nâœ… Pipeline test successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3a2def",
   "metadata": {},
   "source": [
    "## Section 2: Batch Prediction Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431bba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.1: Batch Prediction Function\n",
    "def batch_predict(pipeline: PredictionPipeline, input_list: List[PredictionInput]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Make predictions for multiple inputs\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for input_data in input_list:\n",
    "        try:\n",
    "            output = pipeline.predict(input_data)\n",
    "            results.append(output.dict())\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {input_data.date} - {input_data.lgu_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"âœ… Batch prediction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af665523",
   "metadata": {},
   "source": [
    "## Section 3: Save Production Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e902ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.1: Save Production Model Package\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING PRODUCTION ARTIFACTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Save model\n",
    "model_prod_path = PRODUCTION_DIR / 'model.pkl'\n",
    "joblib.dump(best_model, model_prod_path)\n",
    "print(f\"\\nâœ… Model saved: {model_prod_path}\")\n",
    "\n",
    "# 2. Save pipeline\n",
    "pipeline_path = PRODUCTION_DIR / 'pipeline.pkl'\n",
    "joblib.dump(pipeline, pipeline_path)\n",
    "print(f\"âœ… Pipeline saved: {pipeline_path}\")\n",
    "\n",
    "# 3. Save metadata with production info\n",
    "production_metadata = {\n",
    "    **metadata,\n",
    "    'model_version': pipeline.model_version,\n",
    "    'production_date': datetime.now().isoformat(),\n",
    "    'location_mapping': LOCATION_MAPPING,\n",
    "    'model_file': 'model.pkl',\n",
    "    'pipeline_file': 'pipeline.pkl',\n",
    "    'input_schema': list(PredictionInput.__fields__.keys()),\n",
    "    'output_schema': list(PredictionOutput.__fields__.keys())\n",
    "}\n",
    "\n",
    "metadata_prod_path = PRODUCTION_DIR / 'metadata.json'\n",
    "with open(metadata_prod_path, 'w') as f:\n",
    "    json.dump(production_metadata, f, indent=2)\n",
    "print(f\"âœ… Metadata saved: {metadata_prod_path}\")\n",
    "\n",
    "# 4. Save example input/output\n",
    "example_input = test_input.dict()\n",
    "example_output = result.dict()\n",
    "\n",
    "examples = {\n",
    "    'input_example': example_input,\n",
    "    'output_example': example_output\n",
    "}\n",
    "\n",
    "examples_path = PRODUCTION_DIR / 'examples.json'\n",
    "with open(examples_path, 'w') as f:\n",
    "    json.dump(examples, f, indent=2)\n",
    "print(f\"âœ… Examples saved: {examples_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRODUCTION ARTIFACTS READY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nLocation: {PRODUCTION_DIR}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "for file in PRODUCTION_DIR.glob('*'):\n",
    "    size_mb = file.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  - {file.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9711a4",
   "metadata": {},
   "source": [
    "## Section 4: Generate FastAPI Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.1: Generate FastAPI Application Code\n",
    "fastapi_code = '''# Production FastAPI Application - School Suspension Prediction\n",
    "# Generated from 04_production_deployment.ipynb\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import Optional, List\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"School Suspension Prediction API\",\n",
    "    description=\"Predict school suspensions based on weather and calendar data\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Load model and metadata on startup\n",
    "MODELS_DIR = Path(\"model-training/models/production\")\n",
    "\n",
    "with open(MODELS_DIR / \"metadata.json\", \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "pipeline = joblib.load(MODELS_DIR / \"pipeline.pkl\")\n",
    "\n",
    "print(f\"âœ… Model loaded: {metadata['model_version']}\")\n",
    "\n",
    "# Import Pydantic models from pipeline\n",
    "# (In production, copy the PredictionInput and PredictionOutput classes here)\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\n",
    "        \"message\": \"School Suspension Prediction API\",\n",
    "        \"model_version\": metadata[\"model_version\"],\n",
    "        \"model_type\": metadata[\"best_model\"],\n",
    "        \"status\": \"healthy\"\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"model_version\": metadata[\"model_version\"]\n",
    "    }\n",
    "\n",
    "@app.get(\"/model/info\")\n",
    "async def model_info():\n",
    "    return {\n",
    "        \"model_type\": metadata[\"best_model\"],\n",
    "        \"version\": metadata[\"model_version\"],\n",
    "        \"f2_score\": metadata[\"best_f2\"],\n",
    "        \"recall\": metadata[\"best_recall\"],\n",
    "        \"precision\": metadata[\"best_precision\"],\n",
    "        \"features_count\": metadata[\"final_feature_count\"],\n",
    "        \"production_date\": metadata[\"production_date\"]\n",
    "    }\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionOutput)\n",
    "async def predict(input_data: PredictionInput):\n",
    "    \"\"\"\n",
    "    Make a suspension prediction for a single date and LGU\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = pipeline.predict(input_data)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "@app.post(\"/predict/batch\")\n",
    "async def predict_batch(input_list: List[PredictionInput]):\n",
    "    \"\"\"\n",
    "    Make predictions for multiple inputs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = []\n",
    "        for input_data in input_list:\n",
    "            result = pipeline.predict(input_data)\n",
    "            results.append(result.dict())\n",
    "        return {\"predictions\": results, \"count\": len(results)}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "\n",
    "# Save FastAPI code\n",
    "web_dir = Path('../../web')\n",
    "fastapi_path = web_dir / 'app_production.py'\n",
    "\n",
    "with open(fastapi_path, 'w') as f:\n",
    "    f.write(fastapi_code)\n",
    "\n",
    "print(\"âœ… FastAPI application code generated\")\n",
    "print(f\"   Location: {fastapi_path}\")\n",
    "print(f\"\\nTo run the API:\")\n",
    "print(f\"  cd {web_dir}\")\n",
    "print(f\"  python app_production.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d50c50",
   "metadata": {},
   "source": [
    "## Section 5: Documentation & README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e04a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.1: Generate Production README\n",
    "readme_content = f'''# School Suspension Prediction - Production Model\n",
    "\n",
    "## Model Information\n",
    "- **Model Type**: {metadata['best_model']}\n",
    "- **Version**: {pipeline.model_version}\n",
    "- **Performance**:\n",
    "  - F2 Score: {metadata['best_f2']:.4f}\n",
    "  - Recall: {metadata['best_recall']:.4f}\n",
    "  - Precision: {metadata['best_precision']:.4f}\n",
    "- **Features**: {metadata['final_feature_count']}\n",
    "- **Production Date**: {datetime.now().strftime('%Y-%m-%d')}\n",
    "\n",
    "## Files\n",
    "- `model.pkl`: Trained EasyEnsemble model\n",
    "- `pipeline.pkl`: Complete prediction pipeline with preprocessing\n",
    "- `metadata.json`: Model metadata and configuration\n",
    "- `examples.json`: Input/output examples\n",
    "- `README.md`: This file\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "### Load the Pipeline\n",
    "```python\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Load pipeline\n",
    "pipeline = joblib.load('production/pipeline.pkl')\n",
    "\n",
    "# Make prediction\n",
    "from pydantic import BaseModel\n",
    "\n",
    "input_data = PredictionInput(\n",
    "    date=\"2025-07-15\",\n",
    "    lgu_name=\"Manila\",\n",
    "    hist_precipitation_sum_t1=50.0,\n",
    "    hist_wind_speed_max_t1=45.0,\n",
    "    fcst_precipitation_sum=80.0,\n",
    "    fcst_wind_speed_max=55.0\n",
    ")\n",
    "\n",
    "result = pipeline.predict(input_data)\n",
    "print(f\"Suspension Predicted: {{result.suspension_predicted}}\")\n",
    "print(f\"Probability: {{result.suspension_probability:.2%}}\")\n",
    "```\n",
    "\n",
    "### Run FastAPI Server\n",
    "```bash\n",
    "cd ../../web\n",
    "python app_production.py\n",
    "```\n",
    "\n",
    "Visit: http://localhost:8000/docs for interactive API documentation\n",
    "\n",
    "## API Endpoints\n",
    "\n",
    "### Health Check\n",
    "```bash\n",
    "curl http://localhost:8000/health\n",
    "```\n",
    "\n",
    "### Model Info\n",
    "```bash\n",
    "curl http://localhost:8000/model/info\n",
    "```\n",
    "\n",
    "### Single Prediction\n",
    "```bash\n",
    "curl -X POST http://localhost:8000/predict \\\\\n",
    "  -H \"Content-Type: application/json\" \\\\\n",
    "  -d '{{\n",
    "    \"date\": \"2025-07-15\",\n",
    "    \"lgu_name\": \"Manila\",\n",
    "    \"hist_precipitation_sum_t1\": 50.0,\n",
    "    \"hist_wind_speed_max_t1\": 45.0,\n",
    "    \"fcst_precipitation_sum\": 80.0,\n",
    "    \"fcst_wind_speed_max\": 55.0\n",
    "  }}'\n",
    "```\n",
    "\n",
    "## Input Requirements\n",
    "\n",
    "### Required Fields\n",
    "- `date`: Date in YYYY-MM-DD format\n",
    "- `lgu_name`: LGU name (Manila, Quezon City, etc.)\n",
    "- `hist_precipitation_sum_t1`: Yesterday's precipitation (mm)\n",
    "- `hist_wind_speed_max_t1`: Yesterday's max wind speed (km/h)\n",
    "- `fcst_precipitation_sum`: Today's forecast precipitation (mm)\n",
    "- `fcst_wind_speed_max`: Today's forecast wind speed (km/h)\n",
    "\n",
    "### Optional Fields\n",
    "All other weather and calendar features have defaults\n",
    "\n",
    "## Monitoring\n",
    "\n",
    "Monitor these metrics in production:\n",
    "- Prediction latency\n",
    "- Prediction confidence distribution\n",
    "- Actual vs predicted suspension rate\n",
    "- Feature drift (weather patterns changing over time)\n",
    "\n",
    "## Retraining\n",
    "\n",
    "Retrain when:\n",
    "- Model performance degrades (F2 < 0.50)\n",
    "- New suspension data available (quarterly)\n",
    "- Weather patterns change significantly\n",
    "- New LGUs need to be added\n",
    "\n",
    "## Support\n",
    "For issues or questions, refer to the training notebooks:\n",
    "- `02_eda_and_core_model_all_features.ipynb`: Model training\n",
    "- `03_final_test_evaluation.ipynb`: Model validation\n",
    "- `04_production_deployment.ipynb`: This deployment process\n",
    "'''\n",
    "\n",
    "readme_path = PRODUCTION_DIR / 'README.md'\n",
    "with open(readme_path, 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"âœ… Production README generated\")\n",
    "print(f\"   Location: {readme_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b8691",
   "metadata": {},
   "source": [
    "## Section 6: Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500727d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.1: Production Deployment Summary\n",
    "print(\"=\"*80)\n",
    "print(\"PRODUCTION DEPLOYMENT COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“¦ Production Package Location: {PRODUCTION_DIR}\")\n",
    "\n",
    "print(f\"\\nðŸ“„ Files Generated:\")\n",
    "for file in sorted(PRODUCTION_DIR.glob('*')):\n",
    "    size_mb = file.stat().st_size / (1024 * 1024)\n",
    "    print(f\"   âœ… {file.name}: {size_mb:.2f} MB\")\n",
    "\n",
    "print(f\"\\nðŸ”§ Model Details:\")\n",
    "print(f\"   Type: {metadata['best_model']}\")\n",
    "print(f\"   Version: {pipeline.model_version}\")\n",
    "print(f\"   F2 Score: {metadata['best_f2']:.4f}\")\n",
    "print(f\"   Features: {metadata['final_feature_count']}\")\n",
    "\n",
    "print(f\"\\nðŸš€ Next Steps:\")\n",
    "print(f\"   1. Review the production README: {readme_path}\")\n",
    "print(f\"   2. Test the FastAPI endpoint: cd ../../web && python app_production.py\")\n",
    "print(f\"   3. Deploy to your hosting platform (Supabase, AWS, Azure, etc.)\")\n",
    "print(f\"   4. Set up monitoring and alerting\")\n",
    "print(f\"   5. Schedule retraining pipeline\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ‰ PRODUCTION READY!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
