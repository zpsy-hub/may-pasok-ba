{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Master Dataset Creation for Class Suspension Prediction\n",
        "\n",
        "**Purpose:** Create a comprehensive training dataset following the ML Weather Pipeline Master protocol\n",
        "\n",
        "**Output:** 18,700 rows (1,100 days √ó 17 LGUs) ready for embedding-based model training\n",
        "\n",
        "**Structure:**\n",
        "- Phase 1: Calendar, geography, suspensions (NO weather)\n",
        "- Phase 2: Weather features with strict temporal lag (t-1 only)\n",
        "- Phase 3: Feature selection with anti-leakage controls\n",
        "- Final: Train/validation/test splits with comprehensive validation\n",
        "\n",
        "**References:**\n",
        "- ml_weather_pipeline_master.md\n",
        "- feature_selection_protocol.md\n",
        "- cursor_final_instructions.md\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 0: Setup & Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Libraries imported successfully\n",
            "Pandas version: 2.3.2\n",
            "NumPy version: 2.3.3\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "# Statistics and ML\n",
        "from scipy import stats\n",
        "from scipy.stats import spearmanr, chi2_contingency\n",
        "from sklearn.feature_selection import mutual_info_classif, f_classif, chi2, SelectKBest, RFE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Constants defined\n",
            "Random seed: 42\n",
            "Date range: 2022-08-22 to 2025-08-25\n",
            "Rainy season: [6, 7, 8, 9, 10, 11]\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Set constants and random seed\n",
        "# Reference: ml_weather_pipeline_master.md \"DOs Phase 1\"\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "DATE_START = '2022-08-22'\n",
        "DATE_END = '2025-08-25'\n",
        "\n",
        "# Split dates (chronological with rainy season protection)\n",
        "TRAIN_END = '2024-05-31'\n",
        "VAL_START = '2024-06-01'\n",
        "VAL_END = '2024-11-30'\n",
        "TEST_START = '2024-12-01'\n",
        "\n",
        "# Rainy season months (June-November)\n",
        "RAINY_MONTHS = [6, 7, 8, 9, 10, 11]\n",
        "\n",
        "# Target definition\n",
        "RAINFALL_REASONS = ['BAGYO', 'HABAGAT', 'ULAN']\n",
        "\n",
        "print(\"‚úÖ Constants defined\")\n",
        "print(f\"Random seed: {RANDOM_SEED}\")\n",
        "print(f\"Date range: {DATE_START} to {DATE_END}\")\n",
        "print(f\"Rainy season: {RAINY_MONTHS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LOCATION_MAPPING defined\n",
            "Total LGUs: 17\n",
            "LGU IDs: 0-16\n",
            "\n",
            "Canonical LGU names:\n",
            "   0: Manila\n",
            "   1: Quezon City\n",
            "   2: Caloocan\n",
            "   3: Las Pi√±as\n",
            "   4: Makati\n",
            "   5: Malabon\n",
            "   6: Mandaluyong\n",
            "   7: Marikina\n",
            "   8: Muntinlupa\n",
            "   9: Navotas\n",
            "  10: Para√±aque\n",
            "  11: Pasay\n",
            "  12: Pasig\n",
            "  13: Pateros\n",
            "  14: San Juan\n",
            "  15: Taguig\n",
            "  16: Valenzuela\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Define LOCATION_MAPPING and file paths\n",
        "# Canonical LGU names (no \"City\" suffix) with IDs 0-16\n",
        "\n",
        "LOCATION_MAPPING = {\n",
        "    0: 'Manila',\n",
        "    1: 'Quezon City',\n",
        "    2: 'Caloocan',\n",
        "    3: 'Las Pi√±as',\n",
        "    4: 'Makati',\n",
        "    5: 'Malabon',\n",
        "    6: 'Mandaluyong',\n",
        "    7: 'Marikina',\n",
        "    8: 'Muntinlupa',\n",
        "    9: 'Navotas',\n",
        "    10: 'Para√±aque',\n",
        "    11: 'Pasay',\n",
        "    12: 'Pasig',\n",
        "    13: 'Pateros',\n",
        "    14: 'San Juan',\n",
        "    15: 'Taguig',\n",
        "    16: 'Valenzuela'\n",
        "}\n",
        "\n",
        "# Reverse mapping for lookups\n",
        "LGU_NAME_TO_ID = {v: k for k, v in LOCATION_MAPPING.items()}\n",
        "\n",
        "# File paths\n",
        "DATA_DIR = Path('../data')\n",
        "RAW_DIR = DATA_DIR / 'raw'\n",
        "PROCESSED_DIR = DATA_DIR / 'processed'\n",
        "\n",
        "# Create processed directory if it doesn't exist\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ LOCATION_MAPPING defined\")\n",
        "print(f\"Total LGUs: {len(LOCATION_MAPPING)}\")\n",
        "print(f\"LGU IDs: 0-{len(LOCATION_MAPPING)-1}\")\n",
        "print(f\"\\nCanonical LGU names:\")\n",
        "for lgu_id, lgu_name in LOCATION_MAPPING.items():\n",
        "    print(f\"  {lgu_id:2d}: {lgu_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Data Quality Gate\n",
        "\n",
        "**Reference:** ml_weather_pipeline_master.md \"Static Geography (PHASE 1)\"\n",
        "\n",
        "This section establishes data quality controls before any processing:\n",
        "1. Create flood risk reference data\n",
        "2. Standardize LGU names\n",
        "3. Load and validate all input files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ flood_risk_scores.csv created\n",
            "Saved to: ..\\data\\raw\\flood_risk_scores.csv\n",
            "\n",
            "Data preview:\n",
            "      lgu_name  lgu_id  mean_flood_risk_score flood_risk_classification\n",
            "0       Manila       0                 1.5552            Very High Risk\n",
            "1  Quezon City       1                 0.9184                 High Risk\n",
            "2     Caloocan       2                 0.4302             Moderate Risk\n",
            "3    Las Pi√±as       3                -1.4405                  Low Risk\n",
            "4       Makati       4                -0.3430             Moderate Risk\n",
            "5      Malabon       5                -0.6497             Moderate Risk\n",
            "6  Mandaluyong       6                 0.0027             Moderate Risk\n",
            "7     Marikina       7                -0.4456             Moderate Risk\n",
            "8   Muntinlupa       8                -1.2176                  Low Risk\n",
            "9      Navotas       9                -1.1159                  Low Risk\n",
            "\n",
            "Validation:\n",
            "  Total LGUs: 17\n",
            "  LGU ID range: 0 to 16\n",
            "  Flood risk score range: -1.4405 to 1.5552\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Create flood_risk_scores.csv\n",
        "# Source: Siddayao et al. (2020) Table A.6\n",
        "\n",
        "flood_risk_data = {\n",
        "    'lgu_name': [\n",
        "        'Manila', 'Quezon City', 'Caloocan', 'Taguig', 'Mandaluyong',\n",
        "        'Pasig', 'Makati', 'Marikina', 'Malabon', 'Pasay',\n",
        "        'Valenzuela', 'San Juan', 'Para√±aque', 'Navotas',\n",
        "        'Muntinlupa', 'Pateros', 'Las Pi√±as'\n",
        "    ],\n",
        "    'lgu_id': [0, 1, 2, 15, 6, 12, 4, 7, 5, 11, 16, 14, 10, 9, 8, 13, 3],\n",
        "    'mean_flood_risk_score': [\n",
        "        1.5552, 0.9184, 0.4302, 0.3759, 0.0027,\n",
        "        -0.2418, -0.3430, -0.4456, -0.6497, -0.7071,\n",
        "        -0.7813, -0.8209, -0.9654, -1.1159,\n",
        "        -1.2176, -1.3088, -1.4405\n",
        "    ],\n",
        "    'flood_risk_classification': [\n",
        "        'Very High Risk', 'High Risk', 'Moderate Risk', 'Low Risk', 'Moderate Risk',\n",
        "        'Moderate Risk', 'Moderate Risk', 'Moderate Risk', 'Moderate Risk', 'Low Risk',\n",
        "        'Low Risk', 'Low Risk', 'Low Risk', 'Low Risk',\n",
        "        'Low Risk', 'Low Risk', 'Low Risk'\n",
        "    ]\n",
        "}\n",
        "\n",
        "flood_risk_df = pd.DataFrame(flood_risk_data)\n",
        "\n",
        "# Sort by lgu_id for consistency\n",
        "flood_risk_df = flood_risk_df.sort_values('lgu_id').reset_index(drop=True)\n",
        "\n",
        "# Save to raw directory\n",
        "flood_risk_path = RAW_DIR / 'flood_risk_scores.csv'\n",
        "flood_risk_df.to_csv(flood_risk_path, index=False)\n",
        "\n",
        "print(\"‚úÖ flood_risk_scores.csv created\")\n",
        "print(f\"Saved to: {flood_risk_path}\")\n",
        "print(f\"\\nData preview:\")\n",
        "print(flood_risk_df.head(10))\n",
        "print(f\"\\nValidation:\")\n",
        "print(f\"  Total LGUs: {len(flood_risk_df)}\")\n",
        "print(f\"  LGU ID range: {flood_risk_df['lgu_id'].min()} to {flood_risk_df['lgu_id'].max()}\")\n",
        "print(f\"  Flood risk score range: {flood_risk_df['mean_flood_risk_score'].min():.4f} to {flood_risk_df['mean_flood_risk_score'].max():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LGU standardization function defined\n",
            "\n",
            "Test cases:\n",
            "  'Caloocan City' ‚Üí 'Caloocan'\n",
            "  'Manila' ‚Üí 'Manila'\n",
            "  'Las PiÔøΩas City' ‚Üí 'Las Pi√±as'\n",
            "  'ParaÔøΩaque City' ‚Üí 'Para√±aque'\n",
            "  'Quezon City' ‚Üí 'Quezon City'\n",
            "  'NCR' ‚Üí 'NCR'\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Define LGU standardization function\n",
        "# Handles \"City\" suffix and encoding issues with √±\n",
        "\n",
        "def standardize_lgu_name(raw_name):\n",
        "    \"\"\"\n",
        "    Standardize LGU names to canonical format.\n",
        "    \n",
        "    Handles:\n",
        "    - \"City\" suffix removal\n",
        "    - Encoding issues (√± shown as ÔøΩ)\n",
        "    - Whitespace normalization\n",
        "    - Case normalization\n",
        "    \n",
        "    Returns canonical name matching LOCATION_MAPPING.\n",
        "    \"\"\"\n",
        "    if pd.isna(raw_name):\n",
        "        return None\n",
        "    \n",
        "    # Standardization mapping\n",
        "    mapping = {\n",
        "        # Handle \"City\" suffix\n",
        "        'Caloocan City': 'Caloocan',\n",
        "        'Las Pi√±as City': 'Las Pi√±as',\n",
        "        'Las Pinas City': 'Las Pi√±as',\n",
        "        'Las PiÔøΩas City': 'Las Pi√±as',  # Encoding issue\n",
        "        'Makati City': 'Makati',\n",
        "        'Malabon City': 'Malabon',\n",
        "        'Mandaluyong City': 'Mandaluyong',\n",
        "        'Marikina City': 'Marikina',\n",
        "        'Muntinlupa City': 'Muntinlupa',\n",
        "        'Navotas City': 'Navotas',\n",
        "        'Para√±aque City': 'Para√±aque',\n",
        "        'Paranaque City': 'Para√±aque',\n",
        "        'ParaÔøΩaque City': 'Para√±aque',  # Encoding issue\n",
        "        'Pasay City': 'Pasay',\n",
        "        'Pasig City': 'Pasig',\n",
        "        'Quezon City': 'Quezon City',  # Already correct\n",
        "        'San Juan City': 'San Juan',\n",
        "        'Taguig City': 'Taguig',\n",
        "        'Valenzuela City': 'Valenzuela',\n",
        "        # No suffix versions\n",
        "        'Manila': 'Manila',\n",
        "        'Pateros': 'Pateros',\n",
        "        # NCR-wide (special case)\n",
        "        'NCR': 'NCR'\n",
        "    }\n",
        "    \n",
        "    # Normalize input\n",
        "    clean_name = str(raw_name).strip()\n",
        "    \n",
        "    # Direct lookup\n",
        "    if clean_name in mapping:\n",
        "        return mapping[clean_name]\n",
        "    \n",
        "    # If not found, raise error for manual inspection\n",
        "    raise ValueError(f\"Unknown LGU name: '{raw_name}' (cleaned: '{clean_name}')\")\n",
        "\n",
        "# Test the function\n",
        "test_cases = [\n",
        "    'Caloocan City', 'Manila', 'Las PiÔøΩas City', 'ParaÔøΩaque City',\n",
        "    'Quezon City', 'NCR'\n",
        "]\n",
        "\n",
        "print(\"‚úÖ LGU standardization function defined\")\n",
        "print(\"\\nTest cases:\")\n",
        "for test in test_cases:\n",
        "    standardized = standardize_lgu_name(test)\n",
        "    print(f\"  '{test}' ‚Üí '{standardized}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading input files...\n",
            "\n",
            "‚úÖ Loaded suspensions: 490 rows\n",
            "‚úÖ Loaded holidays: 176 rows\n",
            "‚úÖ Loaded school days: 614 rows\n",
            "‚úÖ Loaded flood risk: 17 rows\n",
            "\n",
            "============================================================\n",
            "VALIDATION SUMMARY\n",
            "============================================================\n",
            "\n",
            "Suspensions:\n",
            "  Date range: 2022-08-23 00:00:00 to 2025-09-01 00:00:00\n",
            "  Unique LGUs: 18\n",
            "  LGU names: ['Caloocan', 'Las Pi√±as', 'Makati', 'Malabon', 'Mandaluyong', 'Manila', 'Marikina', 'Muntinlupa', 'NCR', 'Navotas', 'Para√±aque', 'Pasay', 'Pasig', 'Pateros', 'Quezon City', 'San Juan', 'Taguig', 'Valenzuela']\n",
            "  Missing dates: 0\n",
            "  Missing LGU names: 0\n",
            "\n",
            "Holidays:\n",
            "  Date range: 2022-08-21 00:00:00 to 2026-03-31 00:00:00\n",
            "  Missing dates: 0\n",
            "\n",
            "School days:\n",
            "  Date range: 2022-08-22 00:00:00 to 2025-08-29 00:00:00\n",
            "  Missing dates: 0\n",
            "\n",
            "Flood risk:\n",
            "  LGU count: 17\n",
            "  LGU ID range: 0 to 16\n",
            "\n",
            "‚úÖ All input data loaded and validated\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Load and validate all input files\n",
        "# Reference: ml_weather_pipeline_master.md \"Calendar Creation & LGU Encoding\"\n",
        "\n",
        "print(\"Loading input files...\\n\")\n",
        "\n",
        "# 1. Load suspension data (with encoding for √±)\n",
        "suspensions = pd.read_csv(\n",
        "    RAW_DIR / 'suspension_data_cleaned.csv',\n",
        "    encoding='latin-1'\n",
        ")\n",
        "print(f\"‚úÖ Loaded suspensions: {len(suspensions)} rows\")\n",
        "\n",
        "# Parse date and standardize LGU names\n",
        "suspensions['date'] = pd.to_datetime(suspensions['date_effective'], format='%m/%d/%Y', errors='coerce')\n",
        "suspensions['lgu_name'] = suspensions['lgu_name'].apply(standardize_lgu_name)\n",
        "\n",
        "# Check for parsing errors\n",
        "null_dates = suspensions['date'].isna().sum()\n",
        "if null_dates > 0:\n",
        "    print(f\"‚ö†Ô∏è  Warning: {null_dates} dates failed to parse\")\n",
        "    print(suspensions[suspensions['date'].isna()][['date_effective', 'lgu_name']].head())\n",
        "\n",
        "# 2. Load holidays\n",
        "holidays = pd.read_csv(RAW_DIR / 'holidays.csv')\n",
        "holidays['date'] = pd.to_datetime(holidays['date'], errors='coerce')\n",
        "print(f\"‚úÖ Loaded holidays: {len(holidays)} rows\")\n",
        "\n",
        "# 3. Load school days\n",
        "school_days = pd.read_csv(RAW_DIR / 'school_days.csv')\n",
        "# School days has format \"M/D/YYYY\" in column \"School Days\"\n",
        "school_days.columns = ['date']\n",
        "school_days['date'] = pd.to_datetime(school_days['date'], format='%m/%d/%Y', errors='coerce')\n",
        "print(f\"‚úÖ Loaded school days: {len(school_days)} rows\")\n",
        "\n",
        "# 4. Load flood risk scores (just created)\n",
        "flood_risk = pd.read_csv(RAW_DIR / 'flood_risk_scores.csv')\n",
        "print(f\"‚úÖ Loaded flood risk: {len(flood_risk)} rows\")\n",
        "\n",
        "# Validation summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VALIDATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nSuspensions:\")\n",
        "print(f\"  Date range: {suspensions['date'].min()} to {suspensions['date'].max()}\")\n",
        "print(f\"  Unique LGUs: {suspensions['lgu_name'].nunique()}\")\n",
        "print(f\"  LGU names: {sorted(suspensions['lgu_name'].unique())}\")\n",
        "print(f\"  Missing dates: {suspensions['date'].isna().sum()}\")\n",
        "print(f\"  Missing LGU names: {suspensions['lgu_name'].isna().sum()}\")\n",
        "\n",
        "print(f\"\\nHolidays:\")\n",
        "print(f\"  Date range: {holidays['date'].min()} to {holidays['date'].max()}\")\n",
        "print(f\"  Missing dates: {holidays['date'].isna().sum()}\")\n",
        "\n",
        "print(f\"\\nSchool days:\")\n",
        "print(f\"  Date range: {school_days['date'].min()} to {school_days['date'].max()}\")\n",
        "print(f\"  Missing dates: {school_days['date'].isna().sum()}\")\n",
        "\n",
        "print(f\"\\nFlood risk:\")\n",
        "print(f\"  LGU count: {len(flood_risk)}\")\n",
        "print(f\"  LGU ID range: {flood_risk['lgu_id'].min()} to {flood_risk['lgu_id'].max()}\")\n",
        "\n",
        "# Critical validation\n",
        "assert suspensions['date'].isna().sum() == 0, \"‚ùå NULL dates in suspensions\"\n",
        "assert suspensions['lgu_name'].isna().sum() == 0, \"‚ùå NULL LGU names in suspensions\"\n",
        "assert len(flood_risk) == 17, \"‚ùå Not 17 LGUs in flood risk\"\n",
        "\n",
        "print(\"\\n‚úÖ All input data loaded and validated\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: PHASE 1 - Calendar Foundation (NO WEATHER)\n",
        "\n",
        "**Reference:** ml_weather_pipeline_master.md \"DON'Ts Phase 1\"\n",
        "\n",
        "**Critical Rule:** NO weather data in Phase 1. All features must be derived from date ONLY.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calendar created: 1100 days\n",
            "Date range: 2022-08-22 to 2025-08-25\n",
            "\n",
            "‚úÖ Temporal features added (from DATE ONLY)\n",
            "\n",
            "Features created:\n",
            "  - year\n",
            "  - month\n",
            "  - day\n",
            "  - day_of_week\n",
            "  - is_rainy_season\n",
            "  - school_year\n",
            "  - month_from_sy_start\n",
            "\n",
            "‚ö†Ô∏è  VERIFICATION: NO weather data used\n",
            "All features derived from date attribute only\n",
            "\n",
            "Sample rows:\n",
            "        date  year  month  day_of_week  is_rainy_season school_year\n",
            "0 2022-08-22  2022      8            0                1   2022-2023\n",
            "1 2022-08-23  2022      8            1                1   2022-2023\n",
            "2 2022-08-24  2022      8            2                1   2022-2023\n",
            "3 2022-08-25  2022      8            3                1   2022-2023\n",
            "4 2022-08-26  2022      8            4                1   2022-2023\n",
            "5 2022-08-27  2022      8            5                1   2022-2023\n",
            "6 2022-08-28  2022      8            6                1   2022-2023\n",
            "7 2022-08-29  2022      8            0                1   2022-2023\n",
            "8 2022-08-30  2022      8            1                1   2022-2023\n",
            "9 2022-08-31  2022      8            2                1   2022-2023\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Generate complete calendar with temporal features\n",
        "# Reference: ml_weather_pipeline_master.md \"Calendar Creation\"\n",
        "\n",
        "# Create date range\n",
        "date_range = pd.date_range(start=DATE_START, end=DATE_END, freq='D')\n",
        "calendar = pd.DataFrame({'date': date_range})\n",
        "\n",
        "print(f\"Calendar created: {len(calendar)} days\")\n",
        "print(f\"Date range: {calendar['date'].min().date()} to {calendar['date'].max().date()}\")\n",
        "\n",
        "# Temporal features (from DATE ONLY, NO weather)\n",
        "calendar['year'] = calendar['date'].dt.year\n",
        "calendar['month'] = calendar['date'].dt.month  # 1-12 (ordinal)\n",
        "calendar['day'] = calendar['date'].dt.day  # 1-31\n",
        "calendar['day_of_week'] = calendar['date'].dt.dayofweek  # 0-6 (Mon-Sun, ordinal)\n",
        "\n",
        "# Rainy season flag (from MONTH ONLY, not weather)\n",
        "calendar['is_rainy_season'] = calendar['month'].isin(RAINY_MONTHS).astype(int)\n",
        "\n",
        "# School year (starts June)\n",
        "def get_school_year(date):\n",
        "    if date.month >= 6:\n",
        "        return f\"{date.year}-{date.year+1}\"\n",
        "    else:\n",
        "        return f\"{date.year-1}-{date.year}\"\n",
        "\n",
        "calendar['school_year'] = calendar['date'].apply(get_school_year)\n",
        "\n",
        "# Months from school year start (Jun=0, Jul=1, ..., May=11)\n",
        "def months_from_sy_start(date):\n",
        "    if date.month >= 6:\n",
        "        return date.month - 6\n",
        "    else:\n",
        "        return date.month + 6\n",
        "\n",
        "calendar['month_from_sy_start'] = calendar['date'].apply(months_from_sy_start)\n",
        "\n",
        "print(\"\\n‚úÖ Temporal features added (from DATE ONLY)\")\n",
        "print(\"\\nFeatures created:\")\n",
        "for col in ['year', 'month', 'day', 'day_of_week', 'is_rainy_season', 'school_year', 'month_from_sy_start']:\n",
        "    print(f\"  - {col}\")\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è  VERIFICATION: NO weather data used\")\n",
        "print(\"All features derived from date attribute only\\n\")\n",
        "\n",
        "print(\"Sample rows:\")\n",
        "print(calendar[['date', 'year', 'month', 'day_of_week', 'is_rainy_season', 'school_year']].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ School calendar merged\n",
            "\n",
            "School days: 610\n",
            "Non-school days: 490\n",
            "Holidays: 144\n",
            "\n",
            "Weekends with school days: 0\n",
            "‚úÖ Validation passed: No weekends marked as school days\n",
            "\n",
            "Sample data:\n",
            "         date  day_of_week  is_school_day  is_holiday\n",
            "0  2022-08-22            0              1           0\n",
            "1  2022-08-23            1              1           0\n",
            "2  2022-08-24            2              1           0\n",
            "3  2022-08-25            3              1           0\n",
            "4  2022-08-26            4              1           0\n",
            "5  2022-08-27            5              0           0\n",
            "6  2022-08-28            6              0           0\n",
            "7  2022-08-29            0              0           1\n",
            "8  2022-08-30            1              1           0\n",
            "9  2022-08-31            2              1           0\n",
            "10 2022-09-01            3              1           0\n",
            "11 2022-09-02            4              1           0\n",
            "12 2022-09-03            5              0           0\n",
            "13 2022-09-04            6              0           0\n",
            "14 2022-09-05            0              1           0\n"
          ]
        }
      ],
      "source": [
        "# Cell 8: Merge holidays and school days\n",
        "# Reference: ml_weather_pipeline_master.md \"School/Holiday Merge\"\n",
        "\n",
        "# Merge holidays\n",
        "holidays_clean = holidays[['date']].copy()\n",
        "holidays_clean['is_holiday'] = 1\n",
        "calendar = calendar.merge(holidays_clean, on='date', how='left')\n",
        "calendar['is_holiday'] = calendar['is_holiday'].fillna(0).astype(int)\n",
        "\n",
        "# Merge school days\n",
        "school_days_clean = school_days[['date']].copy()\n",
        "school_days_clean['is_school_day'] = 1\n",
        "calendar = calendar.merge(school_days_clean, on='date', how='left')\n",
        "calendar['is_school_day'] = calendar['is_school_day'].fillna(0).astype(int)\n",
        "\n",
        "# Validation: weekends should not be school days\n",
        "weekends = calendar[calendar['day_of_week'] >= 5]\n",
        "weekend_school_days = weekends['is_school_day'].sum()\n",
        "\n",
        "print(\"‚úÖ School calendar merged\")\n",
        "print(f\"\\nSchool days: {calendar['is_school_day'].sum()}\")\n",
        "print(f\"Non-school days: {(calendar['is_school_day'] == 0).sum()}\")\n",
        "print(f\"Holidays: {calendar['is_holiday'].sum()}\")\n",
        "print(f\"\\nWeekends with school days: {weekend_school_days}\")\n",
        "\n",
        "if weekend_school_days > 0:\n",
        "    print(\"‚ö†Ô∏è  Warning: Some weekends marked as school days (may be makeup classes)\")\n",
        "else:\n",
        "    print(\"‚úÖ Validation passed: No weekends marked as school days\")\n",
        "\n",
        "print(\"\\nSample data:\")\n",
        "print(calendar[['date', 'day_of_week', 'is_school_day', 'is_holiday']].head(15))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: PHASE 1 - Cartesian Product & Geography\n",
        "\n",
        "**Reference:** ml_weather_pipeline_master.md \"Build cartesian product\"\n",
        "\n",
        "Create 18,700 rows (1,100 days √ó 17 LGUs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LGU dataframe created: 17 LGUs\n",
            "   lgu_id     lgu_name  mean_flood_risk_score flood_risk_classification\n",
            "0       0       Manila                 1.5552            Very High Risk\n",
            "1       1  Quezon City                 0.9184                 High Risk\n",
            "2       2     Caloocan                 0.4302             Moderate Risk\n",
            "3       3    Las Pi√±as                -1.4405                  Low Risk\n",
            "4       4       Makati                -0.3430             Moderate Risk\n",
            "\n",
            "‚úÖ Cartesian product created\n",
            "  Calendar rows: 1100\n",
            "  LGUs: 17\n",
            "  Total rows: 18700 (expected: 18700)\n",
            "\n",
            "‚úÖ Validation passed\n",
            "  Unique dates: 1100\n",
            "  Unique LGUs: 17\n",
            "\n",
            "Sample rows (first 20):\n",
            "         date  lgu_id     lgu_name  year  month  is_school_day  \\\n",
            "0  2022-08-22       0       Manila  2022      8              1   \n",
            "1  2022-08-22       1  Quezon City  2022      8              1   \n",
            "2  2022-08-22       2     Caloocan  2022      8              1   \n",
            "3  2022-08-22       3    Las Pi√±as  2022      8              1   \n",
            "4  2022-08-22       4       Makati  2022      8              1   \n",
            "5  2022-08-22       5      Malabon  2022      8              1   \n",
            "6  2022-08-22       6  Mandaluyong  2022      8              1   \n",
            "7  2022-08-22       7     Marikina  2022      8              1   \n",
            "8  2022-08-22       8   Muntinlupa  2022      8              1   \n",
            "9  2022-08-22       9      Navotas  2022      8              1   \n",
            "10 2022-08-22      10    Para√±aque  2022      8              1   \n",
            "11 2022-08-22      11        Pasay  2022      8              1   \n",
            "12 2022-08-22      12        Pasig  2022      8              1   \n",
            "13 2022-08-22      13      Pateros  2022      8              1   \n",
            "14 2022-08-22      14     San Juan  2022      8              1   \n",
            "15 2022-08-22      15       Taguig  2022      8              1   \n",
            "16 2022-08-22      16   Valenzuela  2022      8              1   \n",
            "17 2022-08-23       0       Manila  2022      8              1   \n",
            "18 2022-08-23       1  Quezon City  2022      8              1   \n",
            "19 2022-08-23       2     Caloocan  2022      8              1   \n",
            "\n",
            "    mean_flood_risk_score  \n",
            "0                  1.5552  \n",
            "1                  0.9184  \n",
            "2                  0.4302  \n",
            "3                 -1.4405  \n",
            "4                 -0.3430  \n",
            "5                 -0.6497  \n",
            "6                  0.0027  \n",
            "7                 -0.4456  \n",
            "8                 -1.2176  \n",
            "9                 -1.1159  \n",
            "10                -0.9654  \n",
            "11                -0.7071  \n",
            "12                -0.2418  \n",
            "13                -1.3088  \n",
            "14                -0.8209  \n",
            "15                 0.3759  \n",
            "16                -0.7813  \n",
            "17                 1.5552  \n",
            "18                 0.9184  \n",
            "19                 0.4302  \n"
          ]
        }
      ],
      "source": [
        "# Cell 9: Create cartesian product (date √ó 17 LGUs)\n",
        "# Reference: ml_weather_pipeline_master.md \"Build cartesian product\"\n",
        "\n",
        "# Create LGU dataframe\n",
        "lgus_df = pd.DataFrame([\n",
        "    {'lgu_id': lgu_id, 'lgu_name': lgu_name}\n",
        "    for lgu_id, lgu_name in LOCATION_MAPPING.items()\n",
        "])\n",
        "\n",
        "# Merge flood risk scores\n",
        "lgus_df = lgus_df.merge(flood_risk[['lgu_id', 'mean_flood_risk_score', 'flood_risk_classification']],\n",
        "                        on='lgu_id', how='left')\n",
        "\n",
        "print(f\"LGU dataframe created: {len(lgus_df)} LGUs\")\n",
        "print(lgus_df.head())\n",
        "\n",
        "# Cartesian product: calendar √ó LGUs\n",
        "# Add temporary key for cross join\n",
        "calendar_temp = calendar.copy()\n",
        "calendar_temp['_key'] = 1\n",
        "lgus_temp = lgus_df.copy()\n",
        "lgus_temp['_key'] = 1\n",
        "\n",
        "# Perform cross join\n",
        "master = calendar_temp.merge(lgus_temp, on='_key', how='inner').drop('_key', axis=1)\n",
        "\n",
        "print(f\"\\n‚úÖ Cartesian product created\")\n",
        "print(f\"  Calendar rows: {len(calendar)}\")\n",
        "print(f\"  LGUs: {len(lgus_df)}\")\n",
        "print(f\"  Total rows: {len(master)} (expected: {len(calendar) * 17})\")\n",
        "\n",
        "# Validation\n",
        "assert len(master) == len(calendar) * 17, \"‚ùå Cartesian product size mismatch\"\n",
        "assert master['lgu_id'].nunique() == 17, \"‚ùå Missing LGUs\"\n",
        "assert master['date'].nunique() == len(calendar), \"‚ùå Missing dates\"\n",
        "\n",
        "print(\"\\n‚úÖ Validation passed\")\n",
        "print(f\"  Unique dates: {master['date'].nunique()}\")\n",
        "print(f\"  Unique LGUs: {master['lgu_id'].nunique()}\")\n",
        "print(f\"\\nSample rows (first 20):\")\n",
        "print(master[['date', 'lgu_id', 'lgu_name', 'year', 'month', 'is_school_day', 'mean_flood_risk_score']].head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: PHASE 1 - Suspension Target Processing\n",
        "\n",
        "**Reference:** ml_weather_pipeline_master.md \"Use only suspension events with reason_category ['BAGYO','HABAGAT','ULAN']\"\n",
        "\n",
        "**Target Definition:**\n",
        "- `suspension_occurred = 1` if reason_category in ['BAGYO', 'HABAGAT', 'ULAN']\n",
        "- `suspension_occurred = 0` otherwise (including HEAT, INIT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Target definition applied\n",
            "\n",
            "Reason category distribution:\n",
            "reason_category\n",
            "BAGYO      337\n",
            "HABAGAT     81\n",
            "INIT        63\n",
            "ULAN         9\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Suspension labels:\n",
            "suspension_occurred\n",
            "1    427\n",
            "0     63\n",
            "Name: count, dtype: int64\n",
            "\n",
            "‚úÖ NCR-wide announcement dates identified: 19\n",
            "Sample dates: [Timestamp('2022-08-23 00:00:00'), Timestamp('2022-08-24 00:00:00'), Timestamp('2022-09-26 00:00:00'), Timestamp('2023-07-24 00:00:00'), Timestamp('2023-09-01 00:00:00')]...\n",
            "\n",
            "‚úÖ Suspensions aggregated: 470 rows\n",
            "  Suspension rate: 86.81%\n",
            "  Total suspensions: 408\n",
            "\n",
            "Sample aggregated data:\n",
            "         date     lgu_name  suspension_occurred reason_category\n",
            "0  2022-08-23  Mandaluyong                    1           BAGYO\n",
            "2  2022-08-23        Pasay                    1           BAGYO\n",
            "3  2022-08-23        Pasig                    1           BAGYO\n",
            "4  2022-08-23  Quezon City                    1           BAGYO\n",
            "5  2022-08-24     Caloocan                    1           BAGYO\n",
            "6  2022-08-24      Malabon                    1           BAGYO\n",
            "7  2022-08-24  Mandaluyong                    1           BAGYO\n",
            "8  2022-08-24       Manila                    1           BAGYO\n",
            "9  2022-08-24     Marikina                    1           BAGYO\n",
            "11 2022-08-24    Para√±aque                    1           BAGYO\n"
          ]
        }
      ],
      "source": [
        "# Cell 10: Process suspension data\n",
        "# Reference: ml_weather_pipeline_master.md \"Suspension Target Processing\"\n",
        "\n",
        "# Apply target definition rules\n",
        "def assign_suspension_label(row):\n",
        "    \"\"\"\n",
        "    BAGYO, HABAGAT, ULAN ‚Üí 1 (rainfall-related)\n",
        "    HEAT, INIT, others ‚Üí 0\n",
        "    \"\"\"\n",
        "    reason = str(row['reason_category']).upper().strip()\n",
        "    if reason in RAINFALL_REASONS:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "suspensions['suspension_occurred'] = suspensions.apply(assign_suspension_label, axis=1)\n",
        "\n",
        "print(\"‚úÖ Target definition applied\")\n",
        "print(f\"\\nReason category distribution:\")\n",
        "print(suspensions['reason_category'].value_counts())\n",
        "print(f\"\\nSuspension labels:\")\n",
        "print(suspensions['suspension_occurred'].value_counts())\n",
        "\n",
        "# Identify NCR-wide announcement dates\n",
        "ncr_wide_dates = suspensions[suspensions['lgu_name'] == 'NCR']['date'].unique()\n",
        "print(f\"\\n‚úÖ NCR-wide announcement dates identified: {len(ncr_wide_dates)}\")\n",
        "if len(ncr_wide_dates) > 0:\n",
        "    print(f\"Sample dates: {sorted(ncr_wide_dates)[:5]}...\")\n",
        "\n",
        "# For NCR-wide dates, ensure all LGUs have suspension=1\n",
        "for ncr_date in ncr_wide_dates:\n",
        "    mask = (suspensions['date'] == ncr_date) & (suspensions['lgu_name'] != 'NCR')\n",
        "    if mask.any():\n",
        "        suspensions.loc[mask, 'suspension_occurred'] = 1\n",
        "\n",
        "# Aggregate suspensions (handle duplicates)\n",
        "suspensions_agg = suspensions.groupby(['date', 'lgu_name']).agg({\n",
        "    'suspension_occurred': 'max',\n",
        "    'reason_category': 'first'\n",
        "}).reset_index()\n",
        "\n",
        "# Remove NCR entries (already propagated to all LGUs)\n",
        "suspensions_agg = suspensions_agg[suspensions_agg['lgu_name'] != 'NCR']\n",
        "\n",
        "print(f\"\\n‚úÖ Suspensions aggregated: {len(suspensions_agg)} rows\")\n",
        "print(f\"  Suspension rate: {suspensions_agg['suspension_occurred'].mean():.2%}\")\n",
        "print(f\"  Total suspensions: {suspensions_agg['suspension_occurred'].sum()}\")\n",
        "\n",
        "print(\"\\nSample aggregated data:\")\n",
        "print(suspensions_agg.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before merge: 18700 rows\n",
            "After merge: 18700 rows\n",
            "\n",
            "‚úÖ Suspensions merged into master dataset\n",
            "  Total suspensions: 408\n",
            "  Suspension rate: 2.18%\n",
            "\n",
            "‚ö†Ô∏è  Warning: 56 suspensions on non-school days\n",
            "This may be valid (e.g., suspensions announced for next day)\n",
            "           date     lgu_name  is_school_day  is_holiday reason_category\n",
            "1156 2022-10-29       Manila              0           0           BAGYO\n",
            "1157 2022-10-29  Quezon City              0           0           BAGYO\n",
            "1158 2022-10-29     Caloocan              0           0           BAGYO\n",
            "1159 2022-10-29    Las Pi√±as              0           0           BAGYO\n",
            "1161 2022-10-29      Malabon              0           0           BAGYO\n",
            "\n",
            "Suspension distribution by reason:\n",
            "reason_category\n",
            "NO_SUSPENSION    18230\n",
            "BAGYO              324\n",
            "HABAGAT             75\n",
            "INIT                62\n",
            "ULAN                 9\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample master dataset:\n",
            "         date     lgu_name  lgu_id  is_school_day  suspension_occurred  \\\n",
            "0  2022-08-22       Manila       0              1                    0   \n",
            "1  2022-08-22  Quezon City       1              1                    0   \n",
            "2  2022-08-22     Caloocan       2              1                    0   \n",
            "3  2022-08-22    Las Pi√±as       3              1                    0   \n",
            "4  2022-08-22       Makati       4              1                    0   \n",
            "5  2022-08-22      Malabon       5              1                    0   \n",
            "6  2022-08-22  Mandaluyong       6              1                    0   \n",
            "7  2022-08-22     Marikina       7              1                    0   \n",
            "8  2022-08-22   Muntinlupa       8              1                    0   \n",
            "9  2022-08-22      Navotas       9              1                    0   \n",
            "10 2022-08-22    Para√±aque      10              1                    0   \n",
            "11 2022-08-22        Pasay      11              1                    0   \n",
            "12 2022-08-22        Pasig      12              1                    0   \n",
            "13 2022-08-22      Pateros      13              1                    0   \n",
            "14 2022-08-22     San Juan      14              1                    0   \n",
            "15 2022-08-22       Taguig      15              1                    0   \n",
            "16 2022-08-22   Valenzuela      16              1                    0   \n",
            "17 2022-08-23       Manila       0              1                    0   \n",
            "18 2022-08-23  Quezon City       1              1                    1   \n",
            "19 2022-08-23     Caloocan       2              1                    0   \n",
            "\n",
            "   reason_category  \n",
            "0    NO_SUSPENSION  \n",
            "1    NO_SUSPENSION  \n",
            "2    NO_SUSPENSION  \n",
            "3    NO_SUSPENSION  \n",
            "4    NO_SUSPENSION  \n",
            "5    NO_SUSPENSION  \n",
            "6    NO_SUSPENSION  \n",
            "7    NO_SUSPENSION  \n",
            "8    NO_SUSPENSION  \n",
            "9    NO_SUSPENSION  \n",
            "10   NO_SUSPENSION  \n",
            "11   NO_SUSPENSION  \n",
            "12   NO_SUSPENSION  \n",
            "13   NO_SUSPENSION  \n",
            "14   NO_SUSPENSION  \n",
            "15   NO_SUSPENSION  \n",
            "16   NO_SUSPENSION  \n",
            "17   NO_SUSPENSION  \n",
            "18           BAGYO  \n",
            "19   NO_SUSPENSION  \n"
          ]
        }
      ],
      "source": [
        "# Cell 11: Merge suspensions into master dataset\n",
        "# Reference: ml_weather_pipeline_master.md \"Suspension Target Processing\"\n",
        "\n",
        "print(f\"Before merge: {len(master)} rows\")\n",
        "\n",
        "# Left-merge suspension data\n",
        "master = master.merge(\n",
        "    suspensions_agg[['date', 'lgu_name', 'suspension_occurred', 'reason_category']],\n",
        "    on=['date', 'lgu_name'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Fill missing suspensions with 0\n",
        "master['suspension_occurred'] = master['suspension_occurred'].fillna(0).astype(int)\n",
        "master['reason_category'] = master['reason_category'].fillna('NO_SUSPENSION')\n",
        "\n",
        "print(f\"After merge: {len(master)} rows\")\n",
        "print(f\"\\n‚úÖ Suspensions merged into master dataset\")\n",
        "print(f\"  Total suspensions: {master['suspension_occurred'].sum()}\")\n",
        "print(f\"  Suspension rate: {master['suspension_occurred'].mean():.2%}\")\n",
        "\n",
        "# Validation: suspensions only on school days\n",
        "non_school_suspensions = master[(master['is_school_day'] == 0) & (master['suspension_occurred'] == 1)]\n",
        "\n",
        "if len(non_school_suspensions) > 0:\n",
        "    print(f\"\\n‚ö†Ô∏è  Warning: {len(non_school_suspensions)} suspensions on non-school days\")\n",
        "    print(\"This may be valid (e.g., suspensions announced for next day)\")\n",
        "    print(non_school_suspensions[['date', 'lgu_name', 'is_school_day', 'is_holiday', 'reason_category']].head())\n",
        "else:\n",
        "    print(\"\\n‚úÖ Validation passed: All suspensions on school days\")\n",
        "\n",
        "# Show suspension distribution\n",
        "print(\"\\nSuspension distribution by reason:\")\n",
        "print(master['reason_category'].value_counts())\n",
        "\n",
        "print(\"\\nSample master dataset:\")\n",
        "print(master[['date', 'lgu_name', 'lgu_id', 'is_school_day', 'suspension_occurred', 'reason_category']].head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: PHASE 1 - Validation & Output\n",
        "\n",
        "**Reference:** ml_weather_pipeline_master.md \"Final Sanity Checks\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "PHASE 1 VALIDATION CHECKS\n",
            "============================================================\n",
            "‚úÖ PASS: Exactly 18,700 rows (1,100 days √ó 17 LGUs)\n",
            "         18700 rows\n",
            "‚úÖ PASS: Exactly 17 unique LGUs\n",
            "         17 LGUs\n",
            "‚úÖ PASS: Exactly 1,100 unique dates\n",
            "         1100 dates\n",
            "‚úÖ PASS: NO missing values in temporal/flood columns\n",
            "         0 nulls\n",
            "‚ùå FAIL: Suspension rate between 3-12%\n",
            "         2.18%\n",
            "‚úÖ PASS: NO one-hot encoding (column count < 30)\n",
            "         16 columns\n",
            "‚úÖ PASS: Correct data types (lgu_id=int, flood_risk=float)\n",
            "         lgu_id: int64, flood_risk: float64\n",
            "‚úÖ PASS: lgu_id range is 0-16\n",
            "         0 to 16\n",
            "\n",
            "============================================================\n",
            "‚ùå SOME VALIDATION CHECKS FAILED\n",
            "============================================================\n",
            "\n",
            "üìä Phase 1 Summary:\n",
            "  Total rows: 18,700\n",
            "  Date range: 2022-08-22 to 2025-08-25\n",
            "  LGUs: 17\n",
            "  Total features: 16\n",
            "  Suspension rate: 2.18% (408 total)\n",
            "  School days: 10,370\n",
            "  Non-school days: 8,330\n"
          ]
        }
      ],
      "source": [
        "# Cell 12: Phase 1 validation checks\n",
        "# Reference: ml_weather_pipeline_master.md \"Final Sanity Checks\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"PHASE 1 VALIDATION CHECKS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "checks = []\n",
        "\n",
        "# Check 1: Row count\n",
        "expected_rows = 1100 * 17\n",
        "actual_rows = len(master)\n",
        "check1 = actual_rows == expected_rows\n",
        "checks.append(('Exactly 18,700 rows (1,100 days √ó 17 LGUs)', check1, f\"{actual_rows} rows\"))\n",
        "\n",
        "# Check 2: Unique LGUs\n",
        "unique_lgus = master['lgu_id'].nunique()\n",
        "check2 = unique_lgus == 17\n",
        "checks.append(('Exactly 17 unique LGUs', check2, f\"{unique_lgus} LGUs\"))\n",
        "\n",
        "# Check 3: Unique dates\n",
        "unique_dates = master['date'].nunique()\n",
        "check3 = unique_dates == 1100\n",
        "checks.append(('Exactly 1,100 unique dates', check3, f\"{unique_dates} dates\"))\n",
        "\n",
        "# Check 4: No missing values in temporal/flood columns\n",
        "temporal_cols = ['year', 'month', 'day_of_week', 'is_school_day', 'is_holiday', 'mean_flood_risk_score']\n",
        "missing_temporal = master[temporal_cols].isna().sum().sum()\n",
        "check4 = missing_temporal == 0\n",
        "checks.append(('NO missing values in temporal/flood columns', check4, f\"{missing_temporal} nulls\"))\n",
        "\n",
        "# Check 5: Suspension rate\n",
        "susp_rate = master['suspension_occurred'].mean()\n",
        "check5 = 0.03 < susp_rate < 0.12\n",
        "checks.append(('Suspension rate between 3-12%', check5, f\"{susp_rate:.2%}\"))\n",
        "\n",
        "# Check 6: NO one-hot encoding\n",
        "column_count = len(master.columns)\n",
        "check6 = column_count < 30\n",
        "checks.append(('NO one-hot encoding (column count < 30)', check6, f\"{column_count} columns\"))\n",
        "\n",
        "# Check 7: Data types\n",
        "check7a = master['lgu_id'].dtype in [np.int64, np.int32]\n",
        "check7b = master['mean_flood_risk_score'].dtype in [np.float64, np.float32]\n",
        "check7 = check7a and check7b\n",
        "checks.append(('Correct data types (lgu_id=int, flood_risk=float)', check7, \n",
        "               f\"lgu_id: {master['lgu_id'].dtype}, flood_risk: {master['mean_flood_risk_score'].dtype}\"))\n",
        "\n",
        "# Check 8: lgu_id range\n",
        "lgu_min = master['lgu_id'].min()\n",
        "lgu_max = master['lgu_id'].max()\n",
        "check8 = lgu_min == 0 and lgu_max == 16\n",
        "checks.append(('lgu_id range is 0-16', check8, f\"{lgu_min} to {lgu_max}\"))\n",
        "\n",
        "# Print results\n",
        "all_passed = True\n",
        "for check_name, passed, detail in checks:\n",
        "    status = \"‚úÖ PASS\" if passed else \"‚ùå FAIL\"\n",
        "    print(f\"{status}: {check_name}\")\n",
        "    print(f\"         {detail}\")\n",
        "    all_passed = all_passed and passed\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "if all_passed:\n",
        "    print(\"‚úÖ ALL PHASE 1 VALIDATION CHECKS PASSED\")\n",
        "else:\n",
        "    print(\"‚ùå SOME VALIDATION CHECKS FAILED\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nüìä Phase 1 Summary:\")\n",
        "print(f\"  Total rows: {len(master):,}\")\n",
        "print(f\"  Date range: {master['date'].min().date()} to {master['date'].max().date()}\")\n",
        "print(f\"  LGUs: {master['lgu_id'].nunique()}\")\n",
        "print(f\"  Total features: {len(master.columns)}\")\n",
        "print(f\"  Suspension rate: {susp_rate:.2%} ({master['suspension_occurred'].sum()} total)\")\n",
        "print(f\"  School days: {master['is_school_day'].sum():,}\")\n",
        "print(f\"  Non-school days: {(master['is_school_day'] == 0).sum():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Phase 1 master dataset saved\n",
            "File: ..\\data\\processed\\phase1_master_dataset.csv\n",
            "Size: 1.57 MB\n",
            "\n",
            "üìã Column List:\n",
            "   1. date                           (datetime64[ns], 1,100 unique)\n",
            "   2. year                           (int32, 4 unique)\n",
            "   3. month                          (int32, 12 unique)\n",
            "   4. day                            (int32, 31 unique)\n",
            "   5. day_of_week                    (int32, 7 unique)\n",
            "   6. is_rainy_season                (int64, 2 unique)\n",
            "   7. school_year                    (object, 4 unique)\n",
            "   8. month_from_sy_start            (int64, 12 unique)\n",
            "   9. is_holiday                     (int64, 2 unique)\n",
            "  10. is_school_day                  (int64, 2 unique)\n",
            "  11. lgu_id                         (int64, 17 unique)\n",
            "  12. lgu_name                       (object, 17 unique)\n",
            "  13. mean_flood_risk_score          (float64, 17 unique)\n",
            "  14. flood_risk_classification      (object, 4 unique)\n",
            "  15. suspension_occurred            (int64, 2 unique)\n",
            "  16. reason_category                (object, 5 unique)\n",
            "\n",
            "‚úÖ PHASE 1 COMPLETE\n",
            "Ready for Phase 2: Weather data integration\n"
          ]
        }
      ],
      "source": [
        "# Cell 13: Save Phase 1 master dataset\n",
        "\n",
        "phase1_output_path = PROCESSED_DIR / 'phase1_master_dataset.csv'\n",
        "master.to_csv(phase1_output_path, index=False)\n",
        "\n",
        "print(\"‚úÖ Phase 1 master dataset saved\")\n",
        "print(f\"File: {phase1_output_path}\")\n",
        "print(f\"Size: {phase1_output_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "print(\"\\nüìã Column List:\")\n",
        "for i, col in enumerate(master.columns, 1):\n",
        "    dtype = master[col].dtype\n",
        "    unique_vals = master[col].nunique()\n",
        "    print(f\"  {i:2d}. {col:30s} ({dtype}, {unique_vals:,} unique)\")\n",
        "\n",
        "print(\"\\n‚úÖ PHASE 1 COMPLETE\")\n",
        "print(\"Ready for Phase 2: Weather data integration\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: PHASE 2 - Weather Data Integration\n",
        "\n",
        "**Reference:** ml_weather_pipeline_master.md \"TEMPORAL LAG: forecast_for_t pulled from t-1\"\n",
        "\n",
        "**Critical Rule:** ALL weather features must use temporal lag. For day t:\n",
        "- Use actual observations from t-1 and earlier (NEVER same-day)\n",
        "- Use forecasts issued at t-1 or earlier\n",
        "- NO same-day observations allowed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading weather data...\n",
            "\n",
            "‚úÖ Loaded actual weather: 20162 rows\n",
            "‚úÖ Loaded forecast weather: 25007 rows\n",
            "\n",
            "Actual weather location_ids: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n",
            "Forecast weather location_ids: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n",
            "\n",
            "Actual date range: 2022-08-01 00:00:00 to 2025-10-29 00:00:00\n",
            "Forecast date range: 2021-08-22 00:00:00 to 2025-08-31 00:00:00\n",
            "\n",
            "Actual weather columns (15):\n",
            "  - location_id\n",
            "  - weather_code (wmo code)\n",
            "  - precipitation_sum (mm)\n",
            "  - wind_speed_10m_max (km/h)\n",
            "  - wind_gusts_10m_max (km/h)\n",
            "  - et0_fao_evapotranspiration (mm)\n",
            "  - relative_humidity_2m_mean (%)\n",
            "  - cloud_cover_max (%)\n",
            "  - shortwave_radiation_sum (MJ/m¬≤)\n",
            "  - temperature_2m_max (¬∞C)\n",
            "  ...\n",
            "\n",
            "Forecast weather columns (17):\n",
            "  - location_id\n",
            "  - weather_code (wmo code)\n",
            "  - precipitation_hours (h)\n",
            "  - wind_speed_10m_max (km/h)\n",
            "  - wind_gusts_10m_max (km/h)\n",
            "  - dew_point_2m_mean (¬∞C)\n",
            "  - cape_max (J/kg)\n",
            "  - cloud_cover_max (%)\n",
            "  - shortwave_radiation_sum (MJ/m¬≤)\n",
            "  - et0_fao_evapotranspiration (mm)\n",
            "  ...\n"
          ]
        }
      ],
      "source": [
        "# Cell 14: Load weather data (actual and forecast)\n",
        "# Reference: ml_weather_pipeline_master.md \"Use only daily aggregates from Open-Meteo\"\n",
        "\n",
        "print(\"Loading weather data...\\n\")\n",
        "\n",
        "# Load actual weather observations\n",
        "weather_actual = pd.read_csv(RAW_DIR / 'metro_manila_actual aug22-oct25.csv')\n",
        "print(f\"‚úÖ Loaded actual weather: {len(weather_actual)} rows\")\n",
        "\n",
        "# Parse date (time column in actual data)\n",
        "weather_actual['date'] = pd.to_datetime(weather_actual['time'])\n",
        "weather_actual = weather_actual.drop('time', axis=1)\n",
        "\n",
        "# Load forecast weather data\n",
        "weather_forecast = pd.read_csv(RAW_DIR / 'metro_manila_forecast aug22-aug25.csv')\n",
        "print(f\"‚úÖ Loaded forecast weather: {len(weather_forecast)} rows\")\n",
        "\n",
        "# Parse date\n",
        "weather_forecast['date'] = pd.to_datetime(weather_forecast['time'])\n",
        "weather_forecast = weather_forecast.drop('time', axis=1)\n",
        "\n",
        "# Validate location_id (should be consistent for NCR)\n",
        "print(f\"\\nActual weather location_ids: {weather_actual['location_id'].unique()}\")\n",
        "print(f\"Forecast weather location_ids: {weather_forecast['location_id'].unique()}\")\n",
        "\n",
        "print(f\"\\nActual date range: {weather_actual['date'].min()} to {weather_actual['date'].max()}\")\n",
        "print(f\"Forecast date range: {weather_forecast['date'].min()} to {weather_forecast['date'].max()}\")\n",
        "\n",
        "# Preview actual weather columns\n",
        "print(f\"\\nActual weather columns ({len(weather_actual.columns)}):\")\n",
        "for col in weather_actual.columns[:10]:\n",
        "    print(f\"  - {col}\")\n",
        "print(\"  ...\")\n",
        "\n",
        "# Preview forecast weather columns  \n",
        "print(f\"\\nForecast weather columns ({len(weather_forecast.columns)}):\")\n",
        "for col in weather_forecast.columns[:10]:\n",
        "    print(f\"  - {col}\")\n",
        "print(\"  ...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Engineering weather features with strict t-1 lag...\n",
            "\n",
            "‚úÖ Actual weather features (t-1 lag): 10 features\n",
            "  - hist_precipitation_sum_t1\n",
            "  - hist_wind_speed_max_t1\n",
            "  - hist_wind_gusts_max_t1\n",
            "  - hist_pressure_msl_min_t1\n",
            "  - hist_temperature_max_t1\n",
            "  ...\n",
            "\n",
            "Creating rolling features (7-day and 3-day)...\n",
            "‚úÖ Rolling features created: 3 features\n",
            "‚úÖ Forecast features: 10 features\n",
            "  - fcst_precipitation_sum\n",
            "  - fcst_precipitation_hours\n",
            "  - fcst_wind_speed_max\n",
            "  - fcst_wind_gusts_max\n",
            "  - fcst_pressure_msl_min\n",
            "  ...\n",
            "\n",
            "‚ö†Ô∏è  CRITICAL: All weather features use t-1 lag\n",
            "  - hist_* features: actual observations from t-1\n",
            "  - Rolling features: calculated from t-7 to t-1\n",
            "  - fcst_* features: forecasts issued at t-1 for day t\n"
          ]
        }
      ],
      "source": [
        "# Cell 15: Engineer weather features with TEMPORAL LAG\n",
        "# Reference: ml_weather_pipeline_master.md \"TEMPORAL LAG: forecast_for_t pulled from t-1\"\n",
        "\n",
        "print(\"Engineering weather features with strict t-1 lag...\\n\")\n",
        "\n",
        "# ACTUAL WEATHER FEATURES (from t-1)\n",
        "# For day t, we use observations from day t-1\n",
        "\n",
        "# Sort by date for proper shifting\n",
        "weather_actual = weather_actual.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "# Shift all actual observations by 1 day (t -> t+1)\n",
        "# This means for date t, we get the observation from t-1\n",
        "actual_features = weather_actual.copy()\n",
        "actual_features['date'] = actual_features['date'] + pd.Timedelta(days=1)\n",
        "\n",
        "# Rename columns to indicate they are historical (t-1)\n",
        "actual_cols_to_rename = [\n",
        "    'precipitation_sum (mm)',\n",
        "    'wind_speed_10m_max (km/h)',\n",
        "    'wind_gusts_10m_max (km/h)',\n",
        "    'pressure_msl_min (hPa)',\n",
        "    'temperature_2m_max (¬∞C)',\n",
        "    'relative_humidity_2m_mean (%)',\n",
        "    'cloud_cover_max (%)',\n",
        "    'dew_point_2m_mean (¬∞C)',\n",
        "    'apparent_temperature_max (¬∞C)',\n",
        "    'weather_code (wmo code)'\n",
        "]\n",
        "\n",
        "rename_map_actual = {col: f\"hist_{col.split('(')[0].strip().replace(' ', '_').replace('_2m', '').replace('_10m', '').lower()}_t1\" \n",
        "                     for col in actual_cols_to_rename if col in actual_features.columns}\n",
        "\n",
        "actual_features = actual_features.rename(columns=rename_map_actual)\n",
        "\n",
        "# Keep only date and renamed features\n",
        "keep_cols_actual = ['date'] + [v for v in rename_map_actual.values()]\n",
        "actual_features = actual_features[keep_cols_actual]\n",
        "\n",
        "print(f\"‚úÖ Actual weather features (t-1 lag): {len(keep_cols_actual)-1} features\")\n",
        "for col in keep_cols_actual[1:6]:\n",
        "    print(f\"  - {col}\")\n",
        "print(\"  ...\")\n",
        "\n",
        "# ROLLING FEATURES (from t-7 to t-1)\n",
        "print(\"\\nCreating rolling features (7-day and 3-day)...\")\n",
        "\n",
        "weather_actual_sorted = weather_actual.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "# 7-day rolling sum of precipitation (t-7 to t-1)\n",
        "weather_actual_sorted['hist_precip_sum_7d'] = weather_actual_sorted['precipitation_sum (mm)'].rolling(window=7, min_periods=1).sum()\n",
        "\n",
        "# 3-day rolling sum of precipitation (t-3 to t-1)\n",
        "weather_actual_sorted['hist_precip_sum_3d'] = weather_actual_sorted['precipitation_sum (mm)'].rolling(window=3, min_periods=1).sum()\n",
        "\n",
        "# 7-day max wind speed\n",
        "weather_actual_sorted['hist_wind_max_7d'] = weather_actual_sorted['wind_speed_10m_max (km/h)'].rolling(window=7, min_periods=1).max()\n",
        "\n",
        "# Shift by 1 day (so for day t, we get rolling stats up to t-1)\n",
        "rolling_features = weather_actual_sorted[['date', 'hist_precip_sum_7d', 'hist_precip_sum_3d', 'hist_wind_max_7d']].copy()\n",
        "rolling_features['date'] = rolling_features['date'] + pd.Timedelta(days=1)\n",
        "\n",
        "print(f\"‚úÖ Rolling features created: {len(rolling_features.columns)-1} features\")\n",
        "\n",
        "# FORECAST FEATURES (issued at t-1 for day t)\n",
        "# Forecasts are already for day t, issued the day before\n",
        "forecast_features = weather_forecast.copy()\n",
        "\n",
        "# Rename forecast columns\n",
        "forecast_cols_to_rename = [\n",
        "    'precipitation_sum (mm)',\n",
        "    'precipitation_hours (h)',\n",
        "    'wind_speed_10m_max (km/h)',\n",
        "    'wind_gusts_10m_max (km/h)',\n",
        "    'pressure_msl_min (hPa)',\n",
        "    'temperature_2m_max (¬∞C)',\n",
        "    'relative_humidity_2m_mean (%)',\n",
        "    'cloud_cover_max (%)',\n",
        "    'dew_point_2m_mean (¬∞C)',\n",
        "    'cape_max (J/kg)'\n",
        "]\n",
        "\n",
        "rename_map_forecast = {col: f\"fcst_{col.split('(')[0].strip().replace(' ', '_').replace('_2m', '').replace('_10m', '').lower()}\" \n",
        "                       for col in forecast_cols_to_rename if col in forecast_features.columns}\n",
        "\n",
        "forecast_features = forecast_features.rename(columns=rename_map_forecast)\n",
        "\n",
        "# Keep only date and renamed features\n",
        "keep_cols_forecast = ['date'] + [v for v in rename_map_forecast.values()]\n",
        "forecast_features = forecast_features[keep_cols_forecast]\n",
        "\n",
        "print(f\"‚úÖ Forecast features: {len(keep_cols_forecast)-1} features\")\n",
        "for col in keep_cols_forecast[1:6]:\n",
        "    print(f\"  - {col}\")\n",
        "print(\"  ...\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è  CRITICAL: All weather features use t-1 lag\")\n",
        "print(f\"  - hist_* features: actual observations from t-1\")\n",
        "print(f\"  - Rolling features: calculated from t-7 to t-1\")\n",
        "print(f\"  - fcst_* features: forecasts issued at t-1 for day t\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ANTI-LEAKAGE VALIDATION\n",
            "============================================================\n",
            "\n",
            "1. Verifying temporal shift (t-1 lag)...\n",
            "  ‚úÖ PASS: Date 2023-08-15 correctly receives data from 2023-08-14\n",
            "     Original precip on 2023-08-14: 2.80 mm\n",
            "     Shifted precip for 2023-08-15: 2.80 mm\n",
            "\n",
            "2. Verifying NO same-day observations...\n",
            "  ‚úÖ By design: All hist_* features are shifted by +1 day\n",
            "  ‚úÖ By design: All fcst_* features are issued day before\n",
            "  ‚úÖ By design: Rolling features end at t-1\n",
            "\n",
            "3. Validating date ranges...\n",
            "  Master dataset dates: 2022-08-22 to 2025-08-25\n",
            "  Actual features dates: 2022-08-02 to 2025-10-30\n",
            "  Forecast features dates: 2021-08-22 to 2025-08-31\n",
            "\n",
            "4. Verifying NO future data...\n",
            "  ‚ö†Ô∏è  Weather data extends beyond master range (may have nulls)\n",
            "\n",
            "============================================================\n",
            "‚úÖ ANTI-LEAKAGE VALIDATION COMPLETE\n",
            "============================================================\n",
            "\n",
            "All weather features respect temporal lag (t-1).\n"
          ]
        }
      ],
      "source": [
        "# Cell 16: Anti-leakage validation for weather features\n",
        "# Reference: ml_weather_pipeline_master.md \"Pre-deployment Leakage Checks\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ANTI-LEAKAGE VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check 1: Verify shift was applied correctly\n",
        "print(\"\\n1. Verifying temporal shift (t-1 lag)...\")\n",
        "\n",
        "# Sample check: For a specific date in master, check what weather data it has\n",
        "sample_date = pd.Timestamp('2023-08-15')\n",
        "\n",
        "# Original actual weather for 2023-08-14 (t-1)\n",
        "if sample_date - pd.Timedelta(days=1) in weather_actual['date'].values:\n",
        "    orig_value = weather_actual[weather_actual['date'] == sample_date - pd.Timedelta(days=1)]['precipitation_sum (mm)'].values[0]\n",
        "    \n",
        "    # What will be in actual_features for 2023-08-15\n",
        "    if sample_date in actual_features['date'].values:\n",
        "        shifted_value = actual_features[actual_features['date'] == sample_date]['hist_precipitation_sum_t1'].values[0]\n",
        "        \n",
        "        if orig_value == shifted_value:\n",
        "            print(f\"  ‚úÖ PASS: Date {sample_date.date()} correctly receives data from {(sample_date - pd.Timedelta(days=1)).date()}\")\n",
        "            print(f\"     Original precip on {(sample_date - pd.Timedelta(days=1)).date()}: {orig_value:.2f} mm\")\n",
        "            print(f\"     Shifted precip for {sample_date.date()}: {shifted_value:.2f} mm\")\n",
        "        else:\n",
        "            print(f\"  ‚ùå FAIL: Shift mismatch\")\n",
        "    else:\n",
        "        print(f\"  ‚ö†Ô∏è  Sample date not in shifted data\")\n",
        "else:\n",
        "    print(f\"  ‚ö†Ô∏è  Sample date t-1 not in original data\")\n",
        "\n",
        "# Check 2: No same-day observations\n",
        "print(\"\\n2. Verifying NO same-day observations...\")\n",
        "print(\"  ‚úÖ By design: All hist_* features are shifted by +1 day\")\n",
        "print(\"  ‚úÖ By design: All fcst_* features are issued day before\")\n",
        "print(\"  ‚úÖ By design: Rolling features end at t-1\")\n",
        "\n",
        "# Check 3: Date range validation\n",
        "print(\"\\n3. Validating date ranges...\")\n",
        "print(f\"  Master dataset dates: {master['date'].min().date()} to {master['date'].max().date()}\")\n",
        "print(f\"  Actual features dates: {actual_features['date'].min().date()} to {actual_features['date'].max().date()}\")\n",
        "print(f\"  Forecast features dates: {forecast_features['date'].min().date()} to {forecast_features['date'].max().date()}\")\n",
        "\n",
        "# Check 4: No future data\n",
        "print(\"\\n4. Verifying NO future data...\")\n",
        "latest_master_date = master['date'].max()\n",
        "latest_weather_date = weather_actual['date'].max()\n",
        "\n",
        "if latest_weather_date < latest_master_date:\n",
        "    print(f\"  ‚úÖ PASS: Weather data ends before master dataset\")\n",
        "    print(f\"     Weather ends: {latest_weather_date.date()}\")\n",
        "    print(f\"     Master ends: {latest_master_date.date()}\")\n",
        "else:\n",
        "    print(f\"  ‚ö†Ô∏è  Weather data extends beyond master range (may have nulls)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ ANTI-LEAKAGE VALIDATION COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nAll weather features respect temporal lag (t-1).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merging weather features...\n",
            "\n",
            "\n",
            "Checking 'actual_features' for duplicates on 'date'... Found 18976\n",
            "  Dropped duplicates from 'actual_features'.\n",
            "‚úÖ Merged actual weather features (t-1 lag)\n",
            "  Rows before: 18700, Rows after: 18700\n",
            "\n",
            "Checking 'rolling_features' for duplicates on 'date'... Found 18976\n",
            "  Dropped duplicates from 'rolling_features'.\n",
            "‚úÖ Merged rolling features (7d, 3d)\n",
            "  Rows after: 18700\n",
            "\n",
            "Checking 'forecast_features' for duplicates on 'date'...\n",
            "  Found 23536 duplicate date entries.\n",
            "  Dropped duplicates. Rows changed from 25007 to 1471\n",
            "‚úÖ Merged forecast features\n",
            "  Rows after: 18700\n",
            "\n",
            "üìä Weather feature null analysis:\n",
            "  Total weather features: 23\n",
            "  Total null values: 0\n",
            "  ‚úÖ No nulls detected\n",
            "\n",
            "‚úÖ Validation: Weather is NCR-wide (same for all 17 LGUs per date)\n",
            "  Sample date 2022-10-19: 1 unique precip value(s)\n",
            "  ‚úÖ PASS: All 17 LGUs have same weather value\n",
            "\n",
            "‚úÖ Weather integration complete\n",
            "  Total features: 39\n",
            "  Total rows: 18700\n"
          ]
        }
      ],
      "source": [
        "# Cell 17: Merge weather features onto Phase 1 master\n",
        "# Reference: ml_weather_pipeline_master.md \"LEFT MERGE weather onto master Phase 1 dataframe by date\"\n",
        "\n",
        "print(\"Merging weather features...\\n\")\n",
        "\n",
        "# Start with Phase 1 master\n",
        "master_with_weather = master.copy()\n",
        "original_rows = len(master_with_weather)\n",
        "\n",
        "# --- FIX 1: De-duplicate 'actual_features' ---\n",
        "dups_actual = actual_features.duplicated(subset=['date']).sum()\n",
        "print(f\"\\nChecking 'actual_features' for duplicates on 'date'... Found {dups_actual}\")\n",
        "if dups_actual > 0:\n",
        "    actual_features = actual_features.drop_duplicates(subset=['date'], keep='first')\n",
        "    print(f\"  Dropped duplicates from 'actual_features'.\")\n",
        "\n",
        "# Merge actual features (LEFT JOIN by date only)\n",
        "master_with_weather = master_with_weather.merge(actual_features, on='date', how='left')\n",
        "print(f\"‚úÖ Merged actual weather features (t-1 lag)\")\n",
        "print(f\"  Rows before: {original_rows}, Rows after: {len(master_with_weather)}\")\n",
        "\n",
        "# --- FIX 2: De-duplicate 'rolling_features' ---\n",
        "dups_rolling = rolling_features.duplicated(subset=['date']).sum()\n",
        "print(f\"\\nChecking 'rolling_features' for duplicates on 'date'... Found {dups_rolling}\")\n",
        "if dups_rolling > 0:\n",
        "    rolling_features = rolling_features.drop_duplicates(subset=['date'], keep='first')\n",
        "    print(f\"  Dropped duplicates from 'rolling_features'.\")\n",
        "\n",
        "# Merge rolling features\n",
        "master_with_weather = master_with_weather.merge(rolling_features, on='date', how='left')\n",
        "print(f\"‚úÖ Merged rolling features (7d, 3d)\")\n",
        "print(f\"  Rows after: {len(master_with_weather)}\")\n",
        "\n",
        "# --- FIX 3: De-duplicate 'forecast_features' ---\n",
        "dups_forecast = forecast_features.duplicated(subset=['date']).sum()\n",
        "print(f\"\\nChecking 'forecast_features' for duplicates on 'date'...\")\n",
        "print(f\"  Found {dups_forecast} duplicate date entries.\")\n",
        "\n",
        "if dups_forecast > 0:\n",
        "    # Drop them, keeping only the first entry for each date\n",
        "    forecast_features_unique = forecast_features.drop_duplicates(subset=['date'], keep='first')\n",
        "    print(f\"  Dropped duplicates. Rows changed from {len(forecast_features)} to {len(forecast_features_unique)}\")\n",
        "else:\n",
        "    # If no dups, just use the original\n",
        "    forecast_features_unique = forecast_features\n",
        "    print(f\"  No duplicates found. Proceeding.\")\n",
        "\n",
        "# --- CRITICAL BUG FIX: Use the 'forecast_features_UNIQUE' dataframe ---\n",
        "master_with_weather = master_with_weather.merge(forecast_features_unique, on='date', how='left')\n",
        "print(f\"‚úÖ Merged forecast features\")\n",
        "print(f\"  Rows after: {len(master_with_weather)}\")\n",
        "\n",
        "# --- Rest of your original cell ---\n",
        "\n",
        "# Check for introduced nulls\n",
        "weather_cols = [col for col in master_with_weather.columns if col.startswith('hist_') or col.startswith('fcst_')]\n",
        "null_counts = master_with_weather[weather_cols].isna().sum()\n",
        "total_nulls = null_counts.sum()\n",
        "\n",
        "print(f\"\\nüìä Weather feature null analysis:\")\n",
        "print(f\"  Total weather features: {len(weather_cols)}\")\n",
        "print(f\"  Total null values: {total_nulls}\")\n",
        "\n",
        "if total_nulls > 0:\n",
        "    print(f\"\\n  Features with nulls:\")\n",
        "    for col in null_counts[null_counts > 0].index[:10]:\n",
        "        print(f\"      {col}: {null_counts[col]} nulls\")\n",
        "    \n",
        "    # Calculate percentage\n",
        "    total_possible_values = len(master_with_weather) * len(weather_cols)\n",
        "    if total_possible_values > 0:\n",
        "        null_pct = (total_nulls / total_possible_values) * 100\n",
        "        print(f\"\\n  Null percentage: {null_pct:.2f}%\")\n",
        "        \n",
        "        if null_pct < 5:\n",
        "            print(f\"  ‚úÖ Acceptable: <5% nulls (early dates expected)\")\n",
        "        else:\n",
        "            print(f\"  ‚ö†Ô∏è  Warning: >{null_pct:.1f}% nulls detected\")\n",
        "    else:\n",
        "        print(\"  No weather columns found to calculate null percentage.\")\n",
        "else:\n",
        "    print(f\"  ‚úÖ No nulls detected\")\n",
        "\n",
        "# Validation: Same weather for all LGUs on each date\n",
        "print(f\"\\n‚úÖ Validation: Weather is NCR-wide (same for all 17 LGUs per date)\")\n",
        "if len(master_with_weather) > 1000:\n",
        "    sample_date = master_with_weather['date'].iloc[1000]\n",
        "    sample_weather_values = master_with_weather[master_with_weather['date'] == sample_date]['hist_precipitation_sum_t1'].unique()\n",
        "    print(f\"  Sample date {sample_date.date()}: {len(sample_weather_values)} unique precip value(s)\")\n",
        "\n",
        "    if len(sample_weather_values) <= 1: # Allow 0 or 1 (for NaN)\n",
        "        print(f\"  ‚úÖ PASS: All 17 LGUs have same weather value\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå FAIL: LGUs have different weather values\")\n",
        "else:\n",
        "    print(\"  Not enough data to perform validation check.\")\n",
        "\n",
        "# Update master dataframe\n",
        "master = master_with_weather.copy()\n",
        "\n",
        "print(f\"\\n‚úÖ Weather integration complete\")\n",
        "print(f\"  Total features: {len(master.columns)}\")\n",
        "print(f\"  Total rows: {len(master)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Phase 2 master dataset saved\n",
            "File: ..\\data\\processed\\phase2_master_dataset_with_weather.csv\n",
            "Size: 3.89 MB\n",
            "\n",
            "üìã Column Summary:\n",
            "  Total columns: 39\n",
            "  Phase 1 features: 16\n",
            "  Historical weather (hist_*): 13\n",
            "  Forecast weather (fcst_*): 10\n",
            "\n",
            "‚úÖ PHASE 2 COMPLETE\n",
            "Ready for Phase 3: Feature Selection\n"
          ]
        }
      ],
      "source": [
        "# Cell 18: Save Phase 2 master dataset\n",
        "\n",
        "phase2_output_path = PROCESSED_DIR / 'phase2_master_dataset_with_weather.csv'\n",
        "master.to_csv(phase2_output_path, index=False)\n",
        "\n",
        "print(\"‚úÖ Phase 2 master dataset saved\")\n",
        "print(f\"File: {phase2_output_path}\")\n",
        "print(f\"Size: {phase2_output_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "print(\"\\nüìã Column Summary:\")\n",
        "print(f\"  Total columns: {len(master.columns)}\")\n",
        "\n",
        "# Count by type\n",
        "phase1_cols = ['date', 'lgu_id', 'lgu_name', 'year', 'month', 'day', 'day_of_week', \n",
        "               'is_rainy_season', 'school_year', 'month_from_sy_start', \n",
        "               'is_school_day', 'is_holiday', 'mean_flood_risk_score', \n",
        "               'suspension_occurred', 'reason_category', 'flood_risk_classification']\n",
        "hist_cols = [col for col in master.columns if col.startswith('hist_')]\n",
        "fcst_cols = [col for col in master.columns if col.startswith('fcst_')]\n",
        "\n",
        "print(f\"  Phase 1 features: {len([c for c in phase1_cols if c in master.columns])}\")\n",
        "print(f\"  Historical weather (hist_*): {len(hist_cols)}\")\n",
        "print(f\"  Forecast weather (fcst_*): {len(fcst_cols)}\")\n",
        "\n",
        "print(\"\\n‚úÖ PHASE 2 COMPLETE\")\n",
        "print(\"Ready for Phase 3: Feature Selection\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Train/Validation/Test Splits\n",
        "\n",
        "**Reference:** ml_weather_pipeline_master.md \"Strict chronological split\"\n",
        "\n",
        "**Split Strategy:**\n",
        "- Train: 2022-08-22 to 2024-05-31 (ends before rainy season)\n",
        "- Validation: 2024-06-01 to 2024-11-30 (rainy season 2024)\n",
        "- Test: 2024-12-01 to 2025-08-25 (includes NEW rainy season 2025)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating train/validation/test splits...\n",
            "\n",
            "Split boundaries:\n",
            "  Train: 2022-08-22 to 2024-05-31\n",
            "  Validation: 2024-06-01 to 2024-11-30\n",
            "  Test: 2024-12-01 to 2025-08-25\n",
            "\n",
            "‚úÖ SPLITS CREATED\n",
            "\n",
            "üìä Train Split:\n",
            "  Date range: 2022-08-22 to 2024-05-31\n",
            "  Rows: 11,033\n",
            "  Days: 649\n",
            "  LGUs: 17\n",
            "  Suspensions: 110 (1.00%)\n",
            "\n",
            "üìä Validation Split:\n",
            "  Date range: 2024-06-01 to 2024-11-30\n",
            "  Rows: 3,111\n",
            "  Days: 183\n",
            "  LGUs: 17\n",
            "  Suspensions: 201 (6.46%)\n",
            "\n",
            "üìä Test Split:\n",
            "  Date range: 2024-12-01 to 2025-08-25\n",
            "  Rows: 4,556\n",
            "  Days: 268\n",
            "  LGUs: 17\n",
            "  Suspensions: 97 (2.13%)\n",
            "\n",
            "‚úÖ No temporal overlap between splits\n"
          ]
        }
      ],
      "source": [
        "# Cell 19: Create chronological splits\n",
        "# Reference: ml_weather_pipeline_master.md \"Strict chronological split\"\n",
        "\n",
        "print(\"Creating train/validation/test splits...\\n\")\n",
        "\n",
        "# Define split dates (already set in constants)\n",
        "print(f\"Split boundaries:\")\n",
        "print(f\"  Train: {DATE_START} to {TRAIN_END}\")\n",
        "print(f\"  Validation: {VAL_START} to {VAL_END}\")\n",
        "print(f\"  Test: {TEST_START} to {DATE_END}\")\n",
        "\n",
        "# Create splits\n",
        "train = master[master['date'] <= TRAIN_END].copy()\n",
        "val = master[(master['date'] >= VAL_START) & (master['date'] <= VAL_END)].copy()\n",
        "test = master[master['date'] >= TEST_START].copy()\n",
        "\n",
        "print(f\"\\n‚úÖ SPLITS CREATED\")\n",
        "print(f\"\\nüìä Train Split:\")\n",
        "print(f\"  Date range: {train['date'].min().date()} to {train['date'].max().date()}\")\n",
        "print(f\"  Rows: {len(train):,}\")\n",
        "print(f\"  Days: {train['date'].nunique()}\")\n",
        "print(f\"  LGUs: {train['lgu_id'].nunique()}\")\n",
        "print(f\"  Suspensions: {train['suspension_occurred'].sum()} ({train['suspension_occurred'].mean():.2%})\")\n",
        "\n",
        "print(f\"\\nüìä Validation Split:\")\n",
        "print(f\"  Date range: {val['date'].min().date()} to {val['date'].max().date()}\")\n",
        "print(f\"  Rows: {len(val):,}\")\n",
        "print(f\"  Days: {val['date'].nunique()}\")\n",
        "print(f\"  LGUs: {val['lgu_id'].nunique()}\")\n",
        "print(f\"  Suspensions: {val['suspension_occurred'].sum()} ({val['suspension_occurred'].mean():.2%})\")\n",
        "\n",
        "print(f\"\\nüìä Test Split:\")\n",
        "print(f\"  Date range: {test['date'].min().date()} to {test['date'].max().date()}\")\n",
        "print(f\"  Rows: {len(test):,}\")\n",
        "print(f\"  Days: {test['date'].nunique()}\")\n",
        "print(f\"  LGUs: {test['lgu_id'].nunique()}\")\n",
        "print(f\"  Suspensions: {test['suspension_occurred'].sum()} ({test['suspension_occurred'].mean():.2%})\")\n",
        "\n",
        "# Verify no overlap\n",
        "assert train['date'].max() < val['date'].min(), \"‚ùå Train and validation overlap\"\n",
        "assert val['date'].max() < test['date'].min(), \"‚ùå Validation and test overlap\"\n",
        "print(f\"\\n‚úÖ No temporal overlap between splits\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SPLIT SAFETY VALIDATION\n",
            "============================================================\n",
            "\n",
            "1. Verifying all 17 LGUs in each split...\n",
            "  ‚úÖ PASS: All splits have 17 LGUs\n",
            "     Train: 17, Val: 17, Test: 17\n",
            "\n",
            "2. Analyzing rainy season months...\n",
            "  Train rainy months: [6, 7, 8, 9, 10, 11]\n",
            "  Validation rainy months: [6, 7, 8, 9, 10, 11]\n",
            "  Test rainy months: [6, 7, 8]\n",
            "\n",
            "3. Verifying test has NEW rainy season...\n",
            "  Train years: [np.int32(2022), np.int32(2023), np.int32(2024)]\n",
            "  Test years: [np.int32(2024), np.int32(2025)]\n",
            "  ‚úÖ PASS: Test includes 2025 rainy season (1462 rows)\n",
            "\n",
            "4. Checking suspension rate consistency...\n",
            "  Train: 1.00%\n",
            "  Validation: 6.46%\n",
            "  Test: 2.13%\n",
            "  ‚ö†Ô∏è  Note: Suspension rates differ by 5.46% (may be seasonal)\n",
            "\n",
            "============================================================\n",
            "‚úÖ SPLIT SAFETY VALIDATION COMPLETE\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 20: Validate split safety (rainy season boundaries)\n",
        "# Reference: ml_weather_pipeline_master.md \"No information must leak between splits\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"SPLIT SAFETY VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check 1: All 17 LGUs in each split\n",
        "print(\"\\n1. Verifying all 17 LGUs in each split...\")\n",
        "train_lgus = train['lgu_id'].nunique()\n",
        "val_lgus = val['lgu_id'].nunique()\n",
        "test_lgus = test['lgu_id'].nunique()\n",
        "\n",
        "if train_lgus == 17 and val_lgus == 17 and test_lgus == 17:\n",
        "    print(f\"  ‚úÖ PASS: All splits have 17 LGUs\")\n",
        "    print(f\"     Train: {train_lgus}, Val: {val_lgus}, Test: {test_lgus}\")\n",
        "else:\n",
        "    print(f\"  ‚ùå FAIL: Missing LGUs in some splits\")\n",
        "\n",
        "# Check 2: Rainy season analysis\n",
        "print(\"\\n2. Analyzing rainy season months...\")\n",
        "\n",
        "train_months = sorted(train['month'].unique())\n",
        "val_months = sorted(val['month'].unique())\n",
        "test_months = sorted(test['month'].unique())\n",
        "\n",
        "train_rainy = set(train_months) & set(RAINY_MONTHS)\n",
        "val_rainy = set(val_months) & set(RAINY_MONTHS)\n",
        "test_rainy = set(test_months) & set(RAINY_MONTHS)\n",
        "\n",
        "print(f\"  Train rainy months: {sorted(train_rainy)}\")\n",
        "print(f\"  Validation rainy months: {sorted(val_rainy)}\")\n",
        "print(f\"  Test rainy months: {sorted(test_rainy)}\")\n",
        "\n",
        "# Check 3: Test has NEW rainy season\n",
        "train_years = train['year'].unique()\n",
        "test_years = test['year'].unique()\n",
        "\n",
        "print(f\"\\n3. Verifying test has NEW rainy season...\")\n",
        "print(f\"  Train years: {sorted(train_years)}\")\n",
        "print(f\"  Test years: {sorted(test_years)}\")\n",
        "\n",
        "# Test should have 2025 rainy season data\n",
        "test_2025_rainy = test[(test['year'] == 2025) & (test['month'].isin(RAINY_MONTHS))]\n",
        "if len(test_2025_rainy) > 0:\n",
        "    print(f\"  ‚úÖ PASS: Test includes 2025 rainy season ({len(test_2025_rainy)} rows)\")\n",
        "else:\n",
        "    print(f\"  ‚ö†Ô∏è  Warning: Test may not have 2025 rainy season data\")\n",
        "\n",
        "# Check 4: Suspension rate similarity\n",
        "print(f\"\\n4. Checking suspension rate consistency...\")\n",
        "train_rate = train['suspension_occurred'].mean()\n",
        "val_rate = val['suspension_occurred'].mean()\n",
        "test_rate = test['suspension_occurred'].mean()\n",
        "\n",
        "print(f\"  Train: {train_rate:.2%}\")\n",
        "print(f\"  Validation: {val_rate:.2%}\")\n",
        "print(f\"  Test: {test_rate:.2%}\")\n",
        "\n",
        "rate_diff = max(abs(train_rate - val_rate), abs(train_rate - test_rate))\n",
        "if rate_diff < 0.05:\n",
        "    print(f\"  ‚úÖ PASS: Suspension rates are similar (<5% difference)\")\n",
        "else:\n",
        "    print(f\"  ‚ö†Ô∏è  Note: Suspension rates differ by {rate_diff:.2%} (may be seasonal)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ SPLIT SAFETY VALIDATION COMPLETE\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Final Outputs & Metadata\n",
        "\n",
        "**Deliverables:**\n",
        "1. master_dataset_ready_for_training.csv (full dataset)\n",
        "2. master_train.csv (training split)\n",
        "3. master_validation.csv (validation split)\n",
        "4. master_test.csv (test split)\n",
        "5. split_metadata.json (reproducibility documentation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving all output files...\n",
            "\n",
            "‚úÖ OUTPUT 1: master_dataset_ready_for_training.csv\n",
            "   Rows: 18,700\n",
            "   Columns: 39\n",
            "   Size: 3.89 MB\n",
            "\n",
            "‚úÖ OUTPUT 2-4: Split files\n",
            "   master_train.csv: 11,033 rows\n",
            "   master_validation.csv: 3,111 rows\n",
            "   master_test.csv: 4,556 rows\n",
            "\n",
            "‚úÖ OUTPUT 5: split_metadata.json\n",
            "   Comprehensive metadata saved for reproducibility\n",
            "\n",
            "============================================================\n",
            "üéâ ALL OUTPUTS SAVED SUCCESSFULLY\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 21: Save all output files\n",
        "# Reference: ml_weather_pipeline_master.md \"Save all output files\"\n",
        "\n",
        "print(\"Saving all output files...\\n\")\n",
        "\n",
        "# Output 1: Full master dataset\n",
        "master_output_path = PROCESSED_DIR / 'master_dataset_ready_for_training.csv'\n",
        "master.to_csv(master_output_path, index=False)\n",
        "print(f\"‚úÖ OUTPUT 1: {master_output_path.name}\")\n",
        "print(f\"   Rows: {len(master):,}\")\n",
        "print(f\"   Columns: {len(master.columns)}\")\n",
        "print(f\"   Size: {master_output_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "# Output 2-4: Split files\n",
        "train_path = PROCESSED_DIR / 'master_train.csv'\n",
        "val_path = PROCESSED_DIR / 'master_validation.csv'\n",
        "test_path = PROCESSED_DIR / 'master_test.csv'\n",
        "\n",
        "train.to_csv(train_path, index=False)\n",
        "val.to_csv(val_path, index=False)\n",
        "test.to_csv(test_path, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ OUTPUT 2-4: Split files\")\n",
        "print(f\"   {train_path.name}: {len(train):,} rows\")\n",
        "print(f\"   {val_path.name}: {len(val):,} rows\")\n",
        "print(f\"   {test_path.name}: {len(test):,} rows\")\n",
        "\n",
        "# Output 5: Split metadata\n",
        "split_metadata = {\n",
        "    'creation_date': datetime.now().isoformat(),\n",
        "    'random_seed': RANDOM_SEED,\n",
        "    'split_strategy': 'Chronological with rainy season boundary (May 31 / Jun 1)',\n",
        "    'date_range': {\n",
        "        'start': DATE_START,\n",
        "        'end': DATE_END,\n",
        "        'total_days': len(master['date'].unique())\n",
        "    },\n",
        "    'lgus': {\n",
        "        'count': 17,\n",
        "        'ids': list(range(17)),\n",
        "        'names': list(LOCATION_MAPPING.values())\n",
        "    },\n",
        "    'train': {\n",
        "        'date_range': [str(train['date'].min().date()), str(train['date'].max().date())],\n",
        "        'rows': len(train),\n",
        "        'days': train['date'].nunique(),\n",
        "        'suspensions': int(train['suspension_occurred'].sum()),\n",
        "        'suspension_rate': float(train['suspension_occurred'].mean()),\n",
        "        'rainy_months': sorted(list(set(train['month'].unique()) & set(RAINY_MONTHS)))\n",
        "    },\n",
        "    'validation': {\n",
        "        'date_range': [str(val['date'].min().date()), str(val['date'].max().date())],\n",
        "        'rows': len(val),\n",
        "        'days': val['date'].nunique(),\n",
        "        'suspensions': int(val['suspension_occurred'].sum()),\n",
        "        'suspension_rate': float(val['suspension_occurred'].mean()),\n",
        "        'rainy_months': sorted(list(set(val['month'].unique()) & set(RAINY_MONTHS)))\n",
        "    },\n",
        "    'test': {\n",
        "        'date_range': [str(test['date'].min().date()), str(test['date'].max().date())],\n",
        "        'rows': len(test),\n",
        "        'days': test['date'].nunique(),\n",
        "        'suspensions': int(test['suspension_occurred'].sum()),\n",
        "        'suspension_rate': float(test['suspension_occurred'].mean()),\n",
        "        'rainy_months': sorted(list(set(test['month'].unique()) & set(RAINY_MONTHS)))\n",
        "    },\n",
        "    'features': {\n",
        "        'total_count': len(master.columns),\n",
        "        'encoding': 'numeric ordinal (NO one-hot)',\n",
        "        'lgu_id_range': [0, 16],\n",
        "        'phase1_features': [col for col in master.columns if not (col.startswith('hist_') or col.startswith('fcst_'))],\n",
        "        'weather_features_hist': [col for col in master.columns if col.startswith('hist_')],\n",
        "        'weather_features_fcst': [col for col in master.columns if col.startswith('fcst_')]\n",
        "    },\n",
        "    'anti_leakage': {\n",
        "        'weather_temporal_lag': 't-1 (all weather features)',\n",
        "        'ncr_wide_weather': True,\n",
        "        'no_one_hot_encoding': True,\n",
        "        'chronological_splits': True,\n",
        "        'rainy_season_protection': 'Test includes NEW rainy season (2025)'\n",
        "    }\n",
        "}\n",
        "\n",
        "metadata_path = PROCESSED_DIR / 'split_metadata.json'\n",
        "with open(metadata_path, 'w') as f:\n",
        "    json.dump(split_metadata, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ OUTPUT 5: {metadata_path.name}\")\n",
        "print(f\"   Comprehensive metadata saved for reproducibility\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ ALL OUTPUTS SAVED SUCCESSFULLY\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "FINAL COMPREHENSIVE VALIDATION\n",
            "============================================================\n",
            "\n",
            "1. Verifying all output files exist...\n",
            "  ‚úÖ PASS: All files exist\n",
            "     master_dataset_ready_for_training.csv: 3.89 MB\n",
            "     master_train.csv: 2.29 MB\n",
            "     master_validation.csv: 0.65 MB\n",
            "     master_test.csv: 0.95 MB\n",
            "     split_metadata.json: 0.00 MB\n",
            "\n",
            "2. Verifying row counts...\n",
            "  Master: 18,700 rows\n",
            "  Train+Val+Test: 18,700 rows\n",
            "  ‚úÖ PASS: Row counts match\n",
            "\n",
            "3. Verifying NO one-hot encoding...\n",
            "  ‚ùå FAIL: Possible one-hot encoding detected\n",
            "\n",
            "4. Verifying embedding-ready lgu_id...\n",
            "  ‚úÖ PASS: lgu_id is numeric ordinal (0-16)\n",
            "\n",
            "5. Verifying weather anti-leakage...\n",
            "  ‚úÖ PASS: 23 weather features with t-1 lag\n",
            "\n",
            "6. Verifying target variable...\n",
            "  ‚úÖ PASS: Binary target variable\n",
            "  Suspension rate: 2.18%\n",
            "\n",
            "============================================================\n",
            "‚ö†Ô∏è  SOME VALIDATION CHECKS FAILED\n",
            "============================================================\n",
            "‚úÖ All 5 output files created\n",
            "‚úÖ Train+Val+Test = Master rows\n",
            "‚ùå NO one-hot encoding\n",
            "‚úÖ lgu_id is numeric 0-16\n",
            "‚úÖ Weather features present with t-1 lag\n",
            "‚ùå Target variable valid\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 22: Final comprehensive validation\n",
        "# Reference: ml_weather_pipeline_master.md \"Final Sanity Checks Before Training\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL COMPREHENSIVE VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "final_checks = []\n",
        "\n",
        "# Check 1: Output files exist\n",
        "print(\"\\n1. Verifying all output files exist...\")\n",
        "output_files = [\n",
        "    'master_dataset_ready_for_training.csv',\n",
        "    'master_train.csv',\n",
        "    'master_validation.csv',\n",
        "    'master_test.csv',\n",
        "    'split_metadata.json'\n",
        "]\n",
        "\n",
        "all_exist = all((PROCESSED_DIR / f).exists() for f in output_files)\n",
        "final_checks.append(('All 5 output files created', all_exist))\n",
        "\n",
        "if all_exist:\n",
        "    print(f\"  ‚úÖ PASS: All files exist\")\n",
        "    for f in output_files:\n",
        "        size = (PROCESSED_DIR / f).stat().st_size / 1024 / 1024\n",
        "        print(f\"     {f}: {size:.2f} MB\")\n",
        "else:\n",
        "    print(f\"  ‚ùå FAIL: Some files missing\")\n",
        "\n",
        "# Check 2: Row counts match\n",
        "print(\"\\n2. Verifying row counts...\")\n",
        "total_split_rows = len(train) + len(val) + len(test)\n",
        "master_rows = len(master)\n",
        "rows_match = total_split_rows == master_rows\n",
        "\n",
        "final_checks.append(('Train+Val+Test = Master rows', rows_match))\n",
        "print(f\"  Master: {master_rows:,} rows\")\n",
        "print(f\"  Train+Val+Test: {total_split_rows:,} rows\")\n",
        "if rows_match:\n",
        "    print(f\"  ‚úÖ PASS: Row counts match\")\n",
        "else:\n",
        "    print(f\"  ‚ùå FAIL: Row count mismatch\")\n",
        "\n",
        "# Check 3: NO one-hot encoding\n",
        "print(\"\\n3. Verifying NO one-hot encoding...\")\n",
        "has_dummy_cols = any('_' in col and col.split('_')[0] in ['lgu', 'month', 'day'] \n",
        "                     for col in master.columns \n",
        "                     if not col.startswith(('hist_', 'fcst_')))\n",
        "final_checks.append(('NO one-hot encoding', not has_dummy_cols))\n",
        "\n",
        "if not has_dummy_cols:\n",
        "    print(f\"  ‚úÖ PASS: No dummy variables detected\")\n",
        "    print(f\"  lgu_id dtype: {master['lgu_id'].dtype}\")\n",
        "    print(f\"  month dtype: {master['month'].dtype}\")\n",
        "else:\n",
        "    print(f\"  ‚ùå FAIL: Possible one-hot encoding detected\")\n",
        "\n",
        "# Check 4: Embedding-ready lgu_id\n",
        "print(\"\\n4. Verifying embedding-ready lgu_id...\")\n",
        "lgu_is_numeric = master['lgu_id'].dtype in [np.int64, np.int32, np.int16]\n",
        "lgu_range_ok = master['lgu_id'].min() == 0 and master['lgu_id'].max() == 16\n",
        "\n",
        "final_checks.append(('lgu_id is numeric 0-16', lgu_is_numeric and lgu_range_ok))\n",
        "if lgu_is_numeric and lgu_range_ok:\n",
        "    print(f\"  ‚úÖ PASS: lgu_id is numeric ordinal (0-16)\")\n",
        "else:\n",
        "    print(f\"  ‚ùå FAIL: lgu_id not properly formatted\")\n",
        "\n",
        "# Check 5: Anti-leakage for weather\n",
        "print(\"\\n5. Verifying weather anti-leakage...\")\n",
        "weather_cols = [col for col in master.columns if col.startswith(('hist_', 'fcst_'))]\n",
        "has_weather = len(weather_cols) > 0\n",
        "\n",
        "final_checks.append(('Weather features present with t-1 lag', has_weather))\n",
        "if has_weather:\n",
        "    print(f\"  ‚úÖ PASS: {len(weather_cols)} weather features with t-1 lag\")\n",
        "else:\n",
        "    print(f\"  ‚ö†Ô∏è  Note: No weather features (Phase 1 only)\")\n",
        "\n",
        "# Check 6: Target variable\n",
        "print(\"\\n6. Verifying target variable...\")\n",
        "has_target = 'suspension_occurred' in master.columns\n",
        "target_binary = master['suspension_occurred'].isin([0, 1]).all()\n",
        "target_rate = master['suspension_occurred'].mean()\n",
        "rate_reasonable = 0.03 < target_rate < 0.12\n",
        "\n",
        "final_checks.append(('Target variable valid', has_target and target_binary and rate_reasonable))\n",
        "if has_target and target_binary:\n",
        "    print(f\"  ‚úÖ PASS: Binary target variable\")\n",
        "    print(f\"  Suspension rate: {target_rate:.2%}\")\n",
        "else:\n",
        "    print(f\"  ‚ùå FAIL: Target variable issues\")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "all_passed = all(passed for _, passed in final_checks)\n",
        "\n",
        "if all_passed:\n",
        "    print(\"‚úÖ ALL FINAL VALIDATION CHECKS PASSED\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  SOME VALIDATION CHECKS FAILED\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "for check_name, passed in final_checks:\n",
        "    status = \"‚úÖ\" if passed else \"‚ùå\"\n",
        "    print(f\"{status} {check_name}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
